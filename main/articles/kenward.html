<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Kenward-Roger • mmrm</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Kenward-Roger">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-125641273-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125641273-1');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">mmrm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.13</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Introduction</h6></li>
    <li><a class="dropdown-item" href="../articles/introduction.html">Package Introduction</a></li>
    <li><a class="dropdown-item" href="../articles/methodological_introduction.html">Mixed Models for Repeated Measures</a></li>
    <li><a class="dropdown-item" href="../articles/mmrm_review_methods.html">Comparison with other software</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Covariance Structures</h6></li>
    <li><a class="dropdown-item" href="../articles/covariance.html">Covariance Structures</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Degrees of Freedom and Testing</h6></li>
    <li><a class="dropdown-item" href="../articles/hypothesis_testing.html">Details of Hypothesis Testing</a></li>
    <li><a class="dropdown-item" href="../articles/between_within.html">Between-Within</a></li>
    <li><a class="dropdown-item" href="../articles/kenward.html">Kenward-Roger</a></li>
    <li><a class="dropdown-item" href="../articles/satterthwaite.html">Satterthwaite</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Coefficient Covariance</h6></li>
    <li><a class="dropdown-item" href="../articles/coef_vcov.html">Coefficients Covariance Matrix Adjustment</a></li>
    <li><a class="dropdown-item" href="../articles/empirical_wls.html">Details of Weighted Least Square Empirical Covariance</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Algorithm Details</h6></li>
    <li><a class="dropdown-item" href="../articles/algorithm.html">Model Fitting Algorithm</a></li>
    <li><a class="dropdown-item" href="../articles/predict.html">Prediction and Simulation</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Developers</h6></li>
    <li><a class="dropdown-item" href="../articles/package_structure.html">Package Structure</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
<li class="nav-item dropdown">
<!-- start dropdown for versions --> <li class="nav-item dropdown">
        <a href="#" class="nav-link dropdown-toggle"
          data-bs-toggle="dropdown" role="button"
          aria-expanded="false" aria-haspopup="true"
          id="dropdown-versions">Versions</a> <div class="dropdown-menu" aria-labelledby="dropdown-versions"> <a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/latest-tag">latest-tag</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/main">main</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.12">v0.3.12</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.11">v0.3.11</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.10">v0.3.10</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.7">v0.3.7</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.6">v0.3.6</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.5">v0.3.5</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.4">v0.3.4</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.3">v0.3.3</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.2">v0.3.2</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.1">v0.3.1</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.0">v0.3.0</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.2.2">v0.2.2</a> </div></li>
<!-- end dropdown for versions -->
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-reports" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Reports</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-reports">
<li><a class="dropdown-item" href="../coverage-report/">Coverage report</a></li>
    <li><a class="dropdown-item" href="../unit-test-report/">Unit test report</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/openpharma/mmrm"><span class="fa fa-github"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Kenward-Roger</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/openpharma/mmrm/blob/main/vignettes/kenward.Rmd" class="external-link"><code>vignettes/kenward.Rmd</code></a></small>
      <div class="d-none name"><code>kenward.Rmd</code></div>
    </div>

    
    
<p>Here we describe the details of the calculations for the
Kenward-Roger degrees of freedom and the adjusted covariance matrix of
the coefficients.</p>
<div class="section level3">
<h3 id="model-definition">Model definition<a class="anchor" aria-label="anchor" href="#model-definition"></a>
</h3>
<p>The model definition is the same as what we have in <a href="algorithm.html">Details of the model fitting in
<code>mmrm</code></a>. We are using the same notations.</p>
<div class="section level4">
<h4 id="linear-model">Linear model<a class="anchor" aria-label="anchor" href="#linear-model"></a>
</h4>
<p>For each subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
we observe a vector
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><msub><mi>m</mi><mi>i</mi></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>m</mi><mi>i</mi></msub></msup></mrow><annotation encoding="application/x-tex">
Y_i = (y_{i1}, \dotsc, y_{im_i})^\top \in \mathbb{R}^{m_i}
</annotation></semantics></math> and given a design matrix
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">
X_i \in \mathbb{R}^{m_i \times p}
</annotation></semantics></math> and a corresponding coefficient vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>p</mi></msup></mrow><annotation encoding="application/x-tex">\beta \in \mathbb{R}^{p}</annotation></semantics></math>
we assume that the observations are multivariate normal distributed:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mi>β</mi><mo>,</mo><msub><mi>Σ</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
Y_i \sim N(X_i\beta, \Sigma_i)
</annotation></semantics></math> where the covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Σ</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\Sigma_i \in \mathbb{R}^{m_i \times m_i}</annotation></semantics></math>
is derived by subsetting the overall covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Σ</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\Sigma \in \mathbb{R}^{m \times m}</annotation></semantics></math>
appropriately by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Σ</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>G</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><mi>Σ</mi><msub><mi>S</mi><mi>i</mi></msub><msubsup><mi>G</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">
\Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
</annotation></semantics></math> where the subsetting matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">}</mo><mrow><mi>m</mi><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">S_i \in \{0, 1\}^{m \times m_i}</annotation></semantics></math>
contains in each of its
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding="application/x-tex">m_i</annotation></semantics></math>
columns contains a single 1 indicating which overall time point is
matching
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">t_{ih}</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub><mo>∈</mo><msubsup><mi>ℝ</mi><mrow><mo>&gt;</mo><mn>0</mn></mrow><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msubsup></mrow><annotation encoding="application/x-tex">G_i \in \mathbb{R}_{\gt 0}^{m_i \times m_i}</annotation></semantics></math>
is the diagonal weight matrix.</p>
<p>Conditional on the design matrices
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math>,
the coefficient vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
and the covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
we assume that the observations are independent between the
subjects.</p>
<p>We can write the linear model for all subjects together as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi>β</mi><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">
Y = X\beta + \epsilon
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">Y \in \mathbb{R}^N</annotation></semantics></math>
combines all subject specific observations vectors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Y_i</annotation></semantics></math>
such that we have in total
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">N = \sum_{i = 1}^{n}{m_i}</annotation></semantics></math>
observations,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>N</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in \mathbb{R}^{N \times p}</annotation></semantics></math>
combines all subject specific design matrices and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\epsilon \in \mathbb{R}^N</annotation></semantics></math>
has a multivariate normal distribution
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>Ω</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\epsilon \sim N(0, \Omega)
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\Omega \in \mathbb{R}^{N \times N}</annotation></semantics></math>
is block-diagonal containing the subject specific
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Σ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\Sigma_i</annotation></semantics></math>
covariance matrices on the diagonal and 0 in the remaining entries.</p>
</div>
</div>
<div class="section level3">
<h3 id="mathematical-details-of-kenward-roger-method">Mathematical Details of Kenward-Roger method<a class="anchor" aria-label="anchor" href="#mathematical-details-of-kenward-roger-method"></a>
</h3>
<p>The mathematical derivation of the Kenward-Roger method is based on
the Taylor expansion of the obtained covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\beta</annotation></semantics></math>
to get a more accurate estimate for it. All these derivations are based
on the restricted maximum likelihood. Following the same <a href="algorithm.html#covariance-matrix-model">notation</a>, the
covariance matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>
can be represented as a function of covariance matrix parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\theta = (\theta_1, \dotsc, \theta_k)^\top</annotation></semantics></math>,
i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega(\theta)</annotation></semantics></math>.
Here after model fitting with <code>mmrm</code>, we obtain the estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>X</mi><mi>⊤</mi></msup><mi>Ω</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi></mrow><annotation encoding="application/x-tex">\hat\beta = \Phi(\hat\theta)X^\top\Omega(\hat\theta)^{-1}Y</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">{</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>Ω</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>X</mi><mo stretchy="true" form="postfix">}</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\Phi(\theta) = \left\{X^\top \Omega(\theta)^{-1} X\right\} ^{-1}</annotation></semantics></math>
is the asymptotic covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\beta</annotation></semantics></math>.
However, <span class="citation">Kackar and Harville (1984)</span>
suggests that although the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\beta</annotation></semantics></math>
is unbiased for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>,
the covariance matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">{</mo><msup><mi>X</mi><mi>⊤</mi></msup><mover><mi>Ω</mi><mo accent="true">̂</mo></mover><mi>X</mi><mo stretchy="true" form="postfix">}</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\hat\Phi = \left\{X^\top \hat\Omega X\right\}^{-1}</annotation></semantics></math>
can be biased. They showed that the variability of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\beta</annotation></semantics></math>
can be partitioned into two components,</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Φ</mi><mi>A</mi></msub><mo>=</mo><mi>Φ</mi><mo>+</mo><mi>Λ</mi></mrow><annotation encoding="application/x-tex">
  \Phi_A = \Phi + \Lambda
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Φ</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>
is the variance-covariance matrix of the asymptotic distribution of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\beta</annotation></semantics></math>
as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>→</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">n\rightarrow \infty</annotation></semantics></math>
as defined above, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
represents the amount to which the asymptotic variance-covariance matrix
underestimates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Φ</mi><mi>A</mi></msub><annotation encoding="application/x-tex">\Phi_A</annotation></semantics></math>.</p>
<p>Based on a Taylor series expansion around
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
can be approximated by</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Λ</mi><mo>≃</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">{</mo><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>P</mi><mi>h</mi></msub><mi>Φ</mi><msub><mi>P</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><mo stretchy="true" form="postfix">}</mo></mrow><mi>Φ</mi></mrow><annotation encoding="application/x-tex">
  \Lambda \simeq \Phi \left\{\sum_{h=1}^k{\sum_{j=1}^k{W_{hj}(Q_{hj} - P_h \Phi P_j)} }\right\} \Phi
</annotation></semantics></math> where
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>h</mi></msub><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><mfrac><mrow><mi>∂</mi><msup><mi>Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><mi>X</mi></mrow><annotation encoding="application/x-tex">
  P_h = X^\top \frac{\partial{\Omega^{-1}}}{\partial \theta_h} X
</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><mfrac><mrow><mi>∂</mi><msup><mi>Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><mi>Ω</mi><mfrac><mrow><mi>∂</mi><msup><mi>Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mi>X</mi></mrow><annotation encoding="application/x-tex">
  Q_{hj} = X^\top \frac{\partial{\Omega^{-1}}}{\partial \theta_h} \Omega \frac{\partial{\Omega^{-1}}}{\partial \theta_j} X
</annotation></semantics></math></p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>
is the inverse of the Hessian matrix of the log-likelihood function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
evaluated at the estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>,
i.e. the observed Fisher Information matrix as a consistent estimator of
the variance-covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>.</p>
<p>Again, based on a Taylor series expansion about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
<span class="citation">Kenward and Roger (1997)</span> show that
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mo>≃</mo><mi>Φ</mi><mo>+</mo><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>h</mi></msub><mo>−</mo><msub><mi>θ</mi><mi>h</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mrow><mi>∂</mi><mi>Φ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>h</mi></msub><mo>−</mo><msub><mi>θ</mi><mi>h</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mi>j</mi></msub><mo>−</mo><msub><mi>θ</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>Φ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex">
  \hat\Phi \simeq \Phi + \sum_{h=1}^k{(\hat\theta_h - \theta_h)\frac{\partial{\Phi}}{\partial{\theta_h}}} + \frac{1}{2} \sum_{h=1}^k{\sum_{j=1}^k{(\hat\theta_h - \theta_h)(\hat\theta_j - \theta_j)\frac{\partial^2{\Phi}}{\partial{\theta_h}\partial{\theta_j}}}}
</annotation></semantics></math> Ignoring the possible bias in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>θ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat\theta</annotation></semantics></math>,
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mo>≃</mo><mi>Φ</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>Φ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex">
  E(\hat\Phi) \simeq \Phi + \frac{1}{2} \sum_{h=1}^k{\sum_{j=1}^k{W_{hj}\frac{\partial^2{\Phi}}{\partial{\theta_h}\partial{\theta_j}}}}
</annotation></semantics></math> Using previously defined notations,
this can be further written as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>Φ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mi>Φ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>P</mi><mi>h</mi></msub><mi>Φ</mi><msub><mi>P</mi><mi>j</mi></msub><mo>+</mo><msub><mi>P</mi><mi>j</mi></msub><mi>Φ</mi><msub><mi>P</mi><mi>h</mi></msub><mo>−</mo><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>Q</mi><mrow><mi>j</mi><mi>h</mi></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>Φ</mi></mrow><annotation encoding="application/x-tex">
  \frac{\partial^2{\Phi}}{\partial{\theta_h}\partial{\theta_j}} = \Phi (P_h \Phi P_j + P_j \Phi P_h - Q_{hj} - Q_{jh} + R_{hj}) \Phi
</annotation></semantics></math> where
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi>Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>Ω</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><msup><mi>Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>X</mi></mrow><annotation encoding="application/x-tex">
  R_{hj} = X^\top\Omega^{-1}\frac{\partial^2\Omega}{\partial{\theta_h}\partial{\theta_j}} \Omega^{-1} X
</annotation></semantics></math></p>
<p>substituting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Φ</mi><annotation encoding="application/x-tex">\Phi</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math>
back to the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mi>A</mi></msub><annotation encoding="application/x-tex">\hat\Phi_A</annotation></semantics></math>,
we have</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mi>A</mi></msub><mo>=</mo><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mo>+</mo><mn>2</mn><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">{</mo><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>P</mi><mi>h</mi></msub><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><msub><mi>P</mi><mi>j</mi></msub><mo>−</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow><mo stretchy="true" form="postfix">}</mo></mrow><mover><mi>Φ</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">
  \hat\Phi_A = \hat\Phi + 2\hat\Phi \left\{\sum_{h=1}^k{\sum_{j=1}^k{W_{hj}(Q_{hj} - P_h \hat\Phi P_j - \frac{1}{4}R_{hj})} }\right\} \hat\Phi
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>θ</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega(\hat\theta)</annotation></semantics></math>
replaces
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Omega(\theta)</annotation></semantics></math>
in the right-hand side.</p>
<p>Please note that, if we ignore
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">R_{hj}</annotation></semantics></math>,
the second-order derivatives, we will get a different estimate of
adjusted covariance matrix, and we call this the linear Kenward-Roger
approximation.</p>
<div class="section level4">
<h4 id="special-considerations-for-mmrm-models">Special Considerations for mmrm models<a class="anchor" aria-label="anchor" href="#special-considerations-for-mmrm-models"></a>
</h4>
<p>In mmrm models,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>
is a block-diagonal matrix, hence we can calculate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
for each subject and add them up.</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>h</mi></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>P</mi><mrow><mi>i</mi><mi>h</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><mfrac><mrow><mi>∂</mi><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">
  P_h = \sum_{i=1}^{N}{P_{ih}} = \sum_{i=1}^{N}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} X_i}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>Q</mi><mrow><mi>i</mi><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><mfrac><mrow><mi>∂</mi><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><msub><mi>Σ</mi><mi>i</mi></msub><mfrac><mrow><mi>∂</mi><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">
  Q_{hj} = \sum_{i=1}^{N}{Q_{ihj}} = \sum_{i=1}^{N}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} \Sigma_i \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} X_i}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>R</mi><mrow><mi>i</mi><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><msub><mi>Σ</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">
  R_{hj} = \sum_{i=1}^{N}{R_{ihj}} = \sum_{i=1}^{N}{X_i^\top\Sigma_i^{-1}\frac{\partial^2\Sigma_i}{\partial{\theta_h}\partial{\theta_j}} \Sigma_i^{-1} X_i}
</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="derivative-of-the-overall-covariance-matrix-sigma">Derivative of the overall covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#derivative-of-the-overall-covariance-matrix-sigma"></a>
</h4>
<p>The derivative of the overall covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
with respect to the variance parameters can be calculated through the
derivatives of the Cholesky factor, and hence obtained through automatic
differentiation, following <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Identities_in_differential_form" class="external-link">matrix
identities calculations</a>.
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>Σ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mrow><mi>L</mi><msup><mi>L</mi><mi>⊤</mi></msup></mrow></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><msup><mi>L</mi><mi>⊤</mi></msup><mo>+</mo><mi>L</mi><mfrac><mrow><mi>∂</mi><msup><mi>L</mi><mi>⊤</mi></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">
  \frac{\partial{\Sigma}}{\partial{\theta_h}} = \frac{\partial{LL^\top}}{\partial{\theta_h}} = \frac{\partial{L}}{\partial{\theta_h}}L^\top + L\frac{\partial{L^\top}}{\partial{\theta_h}}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>Σ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>L</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><msup><mi>L</mi><mi>⊤</mi></msup><mo>+</mo><mi>L</mi><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><msup><mi>L</mi><mi>⊤</mi></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><mfrac><mrow><mi>∂</mi><msup><mi>L</mi><mi>T</mi></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>L</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mfrac><mrow><mi>∂</mi><msup><mi>L</mi><mi>⊤</mi></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">
  \frac{\partial^2{\Sigma}}{\partial{\theta_h}\partial{\theta_j}} = \frac{\partial^2{L}}{\partial{\theta_h}\partial{\theta_j}}L^\top + L\frac{\partial^2{L^\top}}{\partial{\theta_h}\partial{\theta_j}} + \frac{\partial{L}}{\partial{\theta_h}}\frac{\partial{L^T}}{\partial{\theta_j}} + \frac{\partial{L}}{\partial{\theta_j}}\frac{\partial{L^\top}}{\partial{\theta_h}}
</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="derivative-of-the-sigma-1">Derivative of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Sigma^{-1}</annotation></semantics></math><a class="anchor" aria-label="anchor" href="#derivative-of-the-sigma-1"></a>
</h4>
<p>The derivatives of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">\Sigma^{-1}</annotation></semantics></math>
can be calculated through</p>
<p><span class="math display">$$
  \frac{\partial{\Sigma\Sigma^{-1}}}{\partial{\theta_h}}\\
  = \frac{\partial{\Sigma}}{\partial{\theta_h}}\Sigma^{-1} +
\Sigma\frac{\partial{\Sigma^{-1}}}{\partial{\theta_h}} \\
  = 0
$$</span>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><msup><mi>Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><mo>=</mo><mi>−</mi><msup><mi>Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mfrac><mrow><mi>∂</mi><mi>Σ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><msup><mi>Σ</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">
  \frac{\partial{\Sigma^{-1}}}{\partial{\theta_h}} = - \Sigma^{-1} \frac{\partial{\Sigma}}{\partial{\theta_h}}\Sigma^{-1}
</annotation></semantics></math></p>
</div>
<div class="section level4">
<h4 id="subjects-with-missed-visits">Subjects with missed visits<a class="anchor" aria-label="anchor" href="#subjects-with-missed-visits"></a>
</h4>
<p>If a subject do not have all visits, the corresponding covariance
matrix can be represented as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Σ</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><mi>Σ</mi><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
  \Sigma_i = S_i^\top \Sigma S_i
</annotation></semantics></math></p>
<p>and the derivatives can be obtained through</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><msub><mi>Σ</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><mo>=</mo><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><mfrac><mrow><mi>∂</mi><mi>Σ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
  \frac{\partial{\Sigma_i}}{\partial{\theta_h}} = S_i^\top \frac{\partial{\Sigma}}{\partial{\theta_h}} S_i
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><msub><mi>Σ</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>Σ</mi></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
  \frac{\partial^2{\Sigma_i}}{\partial{\theta_h}\partial{\theta_j}} = S_i^\top \frac{\partial^2{\Sigma}}{\partial{\theta_h}\partial{\theta_j}} S_i
</annotation></semantics></math></p>
<p>The derivative of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\Sigma_i^{-1}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial\Sigma_i^{-1}}{\partial{\theta_h}}</annotation></semantics></math>
can be calculated through
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><annotation encoding="application/x-tex">\Sigma_i^{-1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><msub><mi>Σ</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial{\Sigma_i}}{\partial{\theta_h}}</annotation></semantics></math>
using the above.</p>
</div>
<div class="section level4">
<h4 id="scenario-under-group-specific-covariance-estimates">Scenario under group specific covariance estimates<a class="anchor" aria-label="anchor" href="#scenario-under-group-specific-covariance-estimates"></a>
</h4>
<p>When fitting grouped <code>mmrm</code> models, the covariance matrix
for subject i of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(i)</annotation></semantics></math>,
can be written as <span class="math display">$$
  \Sigma_i = S_i^\top \Sigma_{g(i)} S_i$.
$$</span> Assume there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
groups, the number of parameters is increased by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
times. With the fact that for each group, the corresponding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
will not affect other parts, we will have block-diagonal
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
matrices, where the blocks are given by:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>h</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>P</mi><mrow><mi>h</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>P</mi><mrow><mi>h</mi><mo>,</mo><mi>B</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
P_h = \begin{pmatrix}
P_{h, 1} &amp; \dots &amp; P_{h, B} \\
\end{pmatrix}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mi>B</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
Q_{hj} = \begin{pmatrix}
Q_{hj, 1} &amp; 0 &amp; \dots &amp; \dots &amp; 0 \\
0 &amp; Q_{hj, 2} &amp; 0 &amp; \dots &amp; 0\\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
\vdots &amp; &amp; &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \dots &amp; 0 &amp; Q_{hj, B}
\end{pmatrix}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mi>B</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
R_{hj} = \begin{pmatrix}
R_{hj, 1} &amp; 0 &amp; \dots &amp; \dots &amp; 0 \\
0 &amp; R_{hj, 2} &amp; 0 &amp; \dots &amp; 0\\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
\vdots &amp; &amp; &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \dots &amp; 0 &amp; R_{hj, B}
\end{pmatrix}
</annotation></semantics></math></p>
<p>Use
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>P</mi><mrow><mi>j</mi><mo>,</mo><mi>b</mi></mrow></msub><annotation encoding="application/x-tex">P_{j, b}</annotation></semantics></math>
to denote the block diagonal part for group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>,
we have
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>h</mi><mo>,</mo><mi>b</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi></mrow></munder><msub><mi>P</mi><mrow><mi>i</mi><mi>h</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi></mrow></munder><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><mfrac><mrow><mi>∂</mi><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">
  P_{h,b} = \sum_{g(i) = b}{P_{ih}} = \sum_{g(i) = b}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} X_i}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mi>b</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi></mrow></munder><msub><mi>Q</mi><mrow><mi>i</mi><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi></mrow></munder><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><mfrac><mrow><mi>∂</mi><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub></mrow></mfrac><msub><mi>Σ</mi><mi>i</mi></msub><mfrac><mrow><mi>∂</mi><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">
  Q_{hj,b} = \sum_{g(i) = b}{Q_{ihj}} = \sum_{g(i) = b}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_h} \Sigma_i \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} X_i}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>h</mi><mi>j</mi><mo>,</mo><mi>b</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi></mrow></munder><msub><mi>R</mi><mrow><mi>i</mi><mi>h</mi><mi>j</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi></mrow></munder><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><msub><mi>Σ</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>θ</mi><mi>h</mi></msub><mi>∂</mi><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac><msubsup><mi>Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup><msub><mi>X</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">
  R_{hj,b} = \sum_{g(i) = b}{R_{ihj}} = \sum_{g(i) = b}{X_i^\top\Sigma_i^{-1}\frac{\partial^2\Sigma_i}{\partial{\theta_h}\partial{\theta_j}} \Sigma_i^{-1} X_i}
</annotation></semantics></math></p>
<p>Similarly for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>.</p>
</div>
<div class="section level4">
<h4 id="scenario-under-weighted-mmrm">Scenario under weighted mmrm<a class="anchor" aria-label="anchor" href="#scenario-under-weighted-mmrm"></a>
</h4>
<p>Under weights mmrm model, the covariance matrix for subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
can be represented as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Σ</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>G</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><mi>Σ</mi><msub><mi>S</mi><mi>i</mi></msub><msubsup><mi>G</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">
  \Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
</annotation></semantics></math></p>
<p>Where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>G</mi><mi>i</mi></msub><annotation encoding="application/x-tex">G_i</annotation></semantics></math>
is a diagonal matrix of the weights. Then, when deriving
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>,
there are no mathematical differences as they are constant, and having
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>G</mi><mi>i</mi></msub><annotation encoding="application/x-tex">G_i</annotation></semantics></math>
in addition to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding="application/x-tex">S_i</annotation></semantics></math>
does not change the algorithms and we can simply multiply the formulas
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>G</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup><annotation encoding="application/x-tex">G_i^{-1/2}</annotation></semantics></math>,
similarly as above for the subsetting matrix.</p>
</div>
</div>
<div class="section level3">
<h3 id="inference">Inference<a class="anchor" aria-label="anchor" href="#inference"></a>
</h3>
<p>Suppose we are testing the linear combination of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>β</mi></mrow><annotation encoding="application/x-tex">C\beta</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>c</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">C \in \mathbb{R}^{c\times p}</annotation></semantics></math>,
we can use the following F-statistic
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mfrac><mn>1</mn><mi>c</mi></mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>β</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mi>C</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>C</mi><mi>⊤</mi></msup><msub><mover><mi>Φ</mi><mo accent="true">̂</mo></mover><mi>A</mi></msub><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>C</mi><mi>⊤</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>−</mo><mi>β</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
  F = \frac{1}{c} (\hat\beta - \beta)^\top  C (C^\top \hat\Phi_A C)^{-1} C^\top (\hat\beta - \beta)
</annotation></semantics></math> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>F</mi><mo>*</mo></msup><mo>=</mo><mi>λ</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">
  F^* = \lambda F
</annotation></semantics></math> follows exact
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>F</mi><mrow><mi>c</mi><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding="application/x-tex">F_{c,m}</annotation></semantics></math>
distribution.</p>
<p>When we have only one coefficient to test, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>
is a column vector containing a single
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>
inside. We can still follow the same calculations as for the
multi-dimensional case. This recovers the degrees of freedom results of
the Satterthwaite method.</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
can be calculated through</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mi>C</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>C</mi><mi>⊤</mi></msup><mi>Φ</mi><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>C</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">
  M = C (C^\top \Phi C)^{-1} C^\top
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mn>1</mn></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mi>t</mi><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mi>Φ</mi><msub><mi>P</mi><mi>h</mi></msub><mi>Φ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>t</mi><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mi>Φ</mi><msub><mi>P</mi><mi>j</mi></msub><mi>Φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">
  A_1 = \sum_{h=1}^k{\sum_{j=1}^k{W_{hj} tr(M \Phi P_h \Phi) tr(M \Phi P_j \Phi)}}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mn>2</mn></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><msub><mi>W</mi><mrow><mi>h</mi><mi>j</mi></mrow></msub><mi>t</mi><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mi>Φ</mi><msub><mi>P</mi><mi>h</mi></msub><mi>Φ</mi><mi>M</mi><mi>Φ</mi><msub><mi>P</mi><mi>j</mi></msub><mi>Φ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">
  A_2 = \sum_{h=1}^k{\sum_{j=1}^k{W_{hj} tr(M \Phi P_h \Phi M \Phi P_j \Phi)}}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>c</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>A</mi><mn>1</mn></msub><mo>+</mo><mn>6</mn><msub><mi>A</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
  B = \frac{1}{2c}(A_1 + 6A_2)
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mn>1</mn></msub><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>+</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mn>2</mn></msub></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>+</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>A</mi><mn>2</mn></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">
  g = \frac{(c+1)A_1 - (c+4)A_2}{(c+2)A_2}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mn>1</mn></msub><mo>=</mo><mfrac><mi>g</mi><mrow><mn>3</mn><mi>c</mi><mo>+</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>g</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">
  c_1 = \frac{g}{3c+2(1-g)}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mn>2</mn></msub><mo>=</mo><mfrac><mrow><mi>c</mi><mo>−</mo><mi>g</mi></mrow><mrow><mn>3</mn><mi>c</mi><mo>+</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>g</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">
  c_2 = \frac{c-g}{3c+2(1-g)}
</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mn>3</mn></msub><mo>=</mo><mfrac><mrow><mi>c</mi><mo>+</mo><mn>2</mn><mo>−</mo><mi>g</mi></mrow><mrow><mn>3</mn><mi>c</mi><mo>+</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>g</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">
  c_3 = \frac{c+2-g}{3c+2(1-g)}
</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>E</mi><mo>*</mo></msup><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">{</mo><mn>1</mn><mo>−</mo><mfrac><msub><mi>A</mi><mn>2</mn></msub><mi>c</mi></mfrac><mo stretchy="true" form="postfix">}</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">E^*={\left\{1-\frac{A_2}{c}\right\}}^{-1}</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mo>*</mo></msup><mo>=</mo><mfrac><mn>2</mn><mi>c</mi></mfrac><mrow><mo stretchy="true" form="prefix">{</mo><mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>c</mi><mn>1</mn></msub><mi>B</mi></mrow><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>c</mi><mn>2</mn></msub><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>c</mi><mn>3</mn></msub><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo stretchy="true" form="postfix">}</mo></mrow></mrow><annotation encoding="application/x-tex">V^*=\frac{2}{c}{\left\{\frac{1+c_1 B}{(1-c_2 B)^2(1-c_3 B)}\right\}}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mo>=</mo><mfrac><msup><mi>V</mi><mo>*</mo></msup><mrow><mn>2</mn><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>E</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\rho = \frac{V^{*}}{2(E^*)^2}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>4</mn><mo>+</mo><mfrac><mrow><mi>c</mi><mo>+</mo><mn>2</mn></mrow><mrow><mi>c</mi><mi>ρ</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">m = 4 + \frac{c+2}{c\rho - 1}</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mfrac><mi>m</mi><mrow><msup><mi>E</mi><mo>*</mo></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mo>−</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">\lambda = \frac{m}{E^*(m-2)}</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="parameterization-methods-and-kenward-roger">Parameterization methods and Kenward-Roger<a class="anchor" aria-label="anchor" href="#parameterization-methods-and-kenward-roger"></a>
</h3>
<p>While the Kenward-Roger adjusted covariance matrix is adopting a
Taylor series to approximate the true value, the choices of
parameterization can change the result. In a simple example of
unstructured covariance structure, in our current approach, where the
parameters are elements of the Cholesky factor of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
(see <a href="covariance.html">parameterization</a>), the second-order
derivatives of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
over our parameters, are non-zero matrices. However, if we use the
elements of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
as our parameters, then the second-order derivatives are zero matrices.
However, the differences can be small and will not affect the inference.
If you would like to match SAS results for the unstructured covariance
model, you can use the linear Kenward-Roger approximation.</p>
</div>
<div class="section level3">
<h3 id="implementations-in-mmrm">Implementations in <code>mmrm</code><a class="anchor" aria-label="anchor" href="#implementations-in-mmrm"></a>
</h3>
<p>In package <code>mmrm</code>, we have implemented Kenward-Roger
calculations based on the previous sections. Specially, for the
first-order and second-order derivatives, we use automatic
differentiation to obtain the results easily for non-spatial covariance
structure. For spatial covariance structure, we derive the exact
results.</p>
<div class="section level4">
<h4 id="spatial-exponential-derivatives">Spatial Exponential Derivatives<a class="anchor" aria-label="anchor" href="#spatial-exponential-derivatives"></a>
</h4>
<p>For spatial exponential covariance structure, we have</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\theta = (\theta_1,\theta_2)</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi><mo>=</mo><msup><mi>e</mi><msub><mi>θ</mi><mn>1</mn></msub></msup></mrow><annotation encoding="application/x-tex">\sigma = e^{\theta_1}</annotation></semantics></math><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>θ</mi><mn>2</mn></msub></msup><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><msub><mi>θ</mi><mn>2</mn></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\rho = \frac{e^{\theta_2}}{1 + e^{\theta_2}}</annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Σ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>σ</mi><msup><mi>ρ</mi><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msup></mrow><annotation encoding="application/x-tex">\Sigma_{ij} = \sigma \rho^{d_{ij}}</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>d</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">d_{ij}</annotation></semantics></math>
is the distance between time point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and time point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>.</p>
<p>So the first-order derivatives can be written as:</p>
<p><span class="math display">$$
  \frac{\partial{\Sigma_{ij}}}{\partial\theta_1} =
\frac{\partial\sigma}{\partial\theta_1} \rho^{d_{ij}}\\
  = e^{\theta_1}\rho^{d_{ij}} \\
  = \Sigma_{ij}
$$</span></p>
<p><span class="math display">$$
  \frac{\partial{\Sigma_{ij}}}{\partial\theta_2} =
\sigma\frac{\partial{\rho^{d_{ij}}}}{\partial\theta_2} \\
  = \sigma\rho^{d_{ij}-1}{d_{ij}}\frac{\partial\rho}{\partial\theta_2}\\
  = \sigma\rho^{d_{ij}-1}{d_{ij}}\rho(1-\rho) \\
  = \sigma \rho^{d_{ij}} {d_{ij}} (1-\rho)
$$</span></p>
<p>Second-order derivatives can be written as:</p>
<p><span class="math display">$$
  \frac{\partial^2{\Sigma_{ij}}}{\partial\theta_1\partial\theta_1}\\
  = \frac{\partial\Sigma_{ij}}{\partial\theta_1}\\
  = \Sigma_{ij}
$$</span></p>
<p><span class="math display">$$
  \frac{\partial^2{\Sigma_{ij}}}{\partial\theta_1\partial\theta_2} =
\frac{\partial^2{\Sigma_{ij}}}{\partial\theta_2\partial\theta_1} \\
  = \frac{\partial\Sigma_{ij}}{\partial\theta_2}\\
  = \sigma\rho^{d_{ij}-1}{d_{ij}}\rho(1-\rho)\\
  = \sigma\rho^{d_{ij}}{d_{ij}}(1-\rho)
$$</span></p>
<p><span class="math display">$$
  \frac{\partial^2{\Sigma_{ij}}}{\partial\theta_2\partial\theta_2}\\
  =
\frac{\partial{\sigma\rho^{d_{ij}}{d_{ij}}(1-\rho)}}{\partial\theta_2}\\
  = \sigma\rho^{d_{ij}}{d_{ij}}(1-\rho)(d_{ij} (1-\rho) - \rho)
$$</span></p>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-kackar1984" class="csl-entry">
Kackar RN, Harville DA (1984). <span>“Approximations for Standard Errors
of Estimators of Fixed and Random Effects in Mixed Linear
Models.”</span> <em>Journal of the American Statistical
Association</em>, <strong>79</strong>(388), 853–862. <a href="https://doi.org/10.1080/01621459.1984.10477102" class="external-link">https://doi.org/10.1080/01621459.1984.10477102.</a>
</div>
<div id="ref-kenward1997" class="csl-entry">
Kenward MG, Roger JH (1997). <span>“Small Sample Inference for Fixed
Effects from Restricted Maximum Likelihood.”</span> <em>Biometrics</em>,
<strong>53</strong>(3), 983–997. <a href="https://doi.org/10.2307/2533558" class="external-link">https://doi.org/10.2307/2533558.</a>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Daniel Sabanes Bove, Liming Li, Julia Dedic, Doug Kelkhoff, Kevin Kunzmann, Brian Matthew Lang, Christian Stock, Ya Wang, Dan James, Jonathan Sidi, Daniel Leibovitz, Daniel D. Sjoberg, Boehringer Ingelheim Ltd., Gilead Sciences, Inc., F. Hoffmann-La Roche AG, Merck Sharp &amp; Dohme, Inc., AstraZeneca plc, inferential.biostatistics GmbH.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
