<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Model Fitting Algorithm • mmrm</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Model Fitting Algorithm">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-125641273-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125641273-1');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">mmrm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.15.9001</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Introduction</h6></li>
    <li><a class="dropdown-item" href="../articles/introduction.html">Package Introduction</a></li>
    <li><a class="dropdown-item" href="../articles/methodological_introduction.html">Mixed Models for Repeated Measures</a></li>
    <li><a class="dropdown-item" href="../articles/mmrm_review_methods.html">Comparison with other software</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Covariance Structures</h6></li>
    <li><a class="dropdown-item" href="../articles/covariance.html">Covariance Structures</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Degrees of Freedom and Testing</h6></li>
    <li><a class="dropdown-item" href="../articles/hypothesis_testing.html">Details of Hypothesis Testing</a></li>
    <li><a class="dropdown-item" href="../articles/between_within.html">Between-Within</a></li>
    <li><a class="dropdown-item" href="../articles/kenward.html">Kenward-Roger</a></li>
    <li><a class="dropdown-item" href="../articles/satterthwaite.html">Satterthwaite</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Coefficient Covariance</h6></li>
    <li><a class="dropdown-item" href="../articles/coef_vcov.html">Coefficients Covariance Matrix Adjustment</a></li>
    <li><a class="dropdown-item" href="../articles/empirical_wls.html">Details of Weighted Least Square Empirical Covariance</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Algorithm Details</h6></li>
    <li><a class="dropdown-item" href="../articles/algorithm.html">Model Fitting Algorithm</a></li>
    <li><a class="dropdown-item" href="../articles/predict.html">Prediction and Simulation</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Developers</h6></li>
    <li><a class="dropdown-item" href="../articles/package_structure.html">Package Structure</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-reports" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Reports</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-reports">
<li><a class="dropdown-item" href="../coverage-report/">Coverage report</a></li>
    <li><a class="dropdown-item" href="../unit-test-report/">Unit test report</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/openpharma/mmrm"><span class="fa fa-github"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Model Fitting Algorithm</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/openpharma/mmrm/blob/main/vignettes/algorithm.Rmd" class="external-link"><code>vignettes/algorithm.Rmd</code></a></small>
      <div class="d-none name"><code>algorithm.Rmd</code></div>
    </div>

    
    
<p>Here we describe the exact model definition as well as the estimation
algorithms in detail. After reading through this vignette, you can
follow the implementation of the algorithm in <code>mmrm.cpp</code> and
the covariance structures in <code>covariance.h</code> in the
<code>src</code> directory of this package.</p>
<div class="section level2">
<h2 id="model-definition">Model definition<a class="anchor" aria-label="anchor" href="#model-definition"></a>
</h2>
<p>The mixed model for repeated measures (MMRM) definition we are using
in this package is the following. Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">i = 1, \dotsc, n</annotation></semantics></math>
denote the subjects from which we observe multiple observations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">j = 1, \dotsc, m_i</annotation></semantics></math>
from total
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding="application/x-tex">m_i</annotation></semantics></math>
time points
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>t</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">t_{ij} \in \{t_1, \dotsc, t_m\}</annotation></semantics></math>.
Note that the number of time points for a specific subject,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding="application/x-tex">m_i</annotation></semantics></math>,
can be smaller than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>,
when only a subset of the possible
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
time points have been observed.</p>
<div class="section level3">
<h3 id="linear-model">Linear model<a class="anchor" aria-label="anchor" href="#linear-model"></a>
</h3>
<p>For each subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
we observe a vector
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>y</mi><mrow><mi>i</mi><msub><mi>m</mi><mi>i</mi></msub></mrow></msub><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>m</mi><mi>i</mi></msub></msup></mrow><annotation encoding="application/x-tex">
Y_i = (y_{i1}, \dotsc, y_{im_i})^\top \in \mathbb{R}^{m_i}
</annotation></semantics></math> and given a design matrix
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">
X_i \in \mathbb{R}^{m_i \times p}
</annotation></semantics></math> and a corresponding coefficient vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>p</mi></msup></mrow><annotation encoding="application/x-tex">\beta \in \mathbb{R}^{p}</annotation></semantics></math>
we assume that the observations are multivariate normal distributed:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>∼</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>X</mi><mi>i</mi></msub><mi>β</mi><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
Y_i \sim N(X_i\beta, \Sigma_i)
</annotation></semantics></math> where the covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\Sigma_i \in \mathbb{R}^{m_i \times m_i}</annotation></semantics></math>
is derived by subsetting the overall covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Σ</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\Sigma \in \mathbb{R}^{m \times m}</annotation></semantics></math>
appropriately by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>G</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><mi mathvariant="normal">Σ</mi><msub><mi>S</mi><mi>i</mi></msub><msubsup><mi>G</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">
\Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
</annotation></semantics></math> where the subsetting matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo stretchy="false" form="postfix">}</mo><mrow><mi>m</mi><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">S_i \in \{0, 1\}^{m \times m_i}</annotation></semantics></math>
contains in each of its
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding="application/x-tex">m_i</annotation></semantics></math>
columns contains a single 1 indicating which overall time point is
matching
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">t_{ij}</annotation></semantics></math>.
Each row contains at most a single 1 but can also contain only 0 if this
time point was not observed. For example, assume a subject was observed
on time points
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">1, 3, 4</annotation></semantics></math>
out of total
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>
then the subsetting matrix is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
S_i = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{pmatrix}.
</annotation></semantics></math><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>i</mi></msub><mo>∈</mo><msubsup><mi>ℝ</mi><mrow><mo>&gt;</mo><mn>0</mn></mrow><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>×</mo><msub><mi>m</mi><mi>i</mi></msub></mrow></msubsup></mrow><annotation encoding="application/x-tex">G_i \in \mathbb{R}_{\gt 0}^{m_i \times m_i}</annotation></semantics></math>
is the diagonal weight matrix, which is the identity matrix if no
weights are specified. Note that this follows from the well known
property of the multivariate normal distribution that linear
combinations of the random vector again have a multivariate normal
distribution with the correspondingly modified mean vector and
covariance matrix.</p>
<p>Conditional on the design matrices
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding="application/x-tex">X_i</annotation></semantics></math>,
the coefficient vector
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
and the covariance matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Σ</mi><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math>
we assume that the observations are independent between the
subjects.</p>
<p>We can write the linear model for all subjects together as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mi>β</mi><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">
Y = X\beta + \epsilon
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">Y \in \mathbb{R}^N</annotation></semantics></math>
combines all subject specific observations vectors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Y_i</annotation></semantics></math>
such that we have in total
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">N = \sum_{i = 1}^{n}{m_i}</annotation></semantics></math>
observations,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>N</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in \mathbb{R}^{N \times p}</annotation></semantics></math>
combines all subject specific design matrices and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\epsilon \in \mathbb{R}^N</annotation></semantics></math>
has a multivariate normal distribution
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mo stretchy="false" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
\epsilon \sim N(0, \Omega)
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ω</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\Omega \in \mathbb{R}^{N \times N}</annotation></semantics></math>
is block-diagonal containing the subject specific
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\Sigma_i</annotation></semantics></math>
covariance matrices on the diagonal and 0 in the remaining entries.</p>
</div>
<div class="section level3">
<h3 id="covariance-matrix-model">Covariance matrix model<a class="anchor" aria-label="anchor" href="#covariance-matrix-model"></a>
</h3>
<p>The symmetric and positive definite covariance matrix
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mn>1</mn><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mn>12</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mn>1</mn><mi>m</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mn>21</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mn>2</mn><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mn>23</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mn>2</mn><mi>m</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mi>m</mi><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>…</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>σ</mi><mrow><mi>m</mi><mo>,</mo><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>σ</mi><mi>m</mi><mn>2</mn></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\Sigma = \begin{pmatrix}
\sigma_1^2 &amp; \sigma_{12} &amp; \dots &amp; \dots &amp; \sigma_{1m} \\
\sigma_{21} &amp; \sigma_2^2 &amp; \sigma_{23} &amp; \dots &amp; \sigma_{2m}\\
\vdots &amp; &amp; \ddots &amp; &amp; \vdots \\
\vdots &amp; &amp; &amp; \ddots &amp; \vdots \\
\sigma_{m1} &amp; \dots &amp; \dots &amp; \sigma_{m,m-1} &amp; \sigma_m^2
\end{pmatrix}
</annotation></semantics></math> is parametrized by a vector of variance
parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>θ</mi><mi>k</mi></msub><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\theta = (\theta_1, \dotsc, \theta_k)^\top</annotation></semantics></math>.
There are many different choices for how to model the covariance matrix
and correspondingly
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
has different interpretations. Since any covariance matrix has a unique
Cholesky factorization
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\Sigma = LL^\top</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math>
is the lower triangular Cholesky factor, we are going to use this
below.</p>
<div class="section level4">
<h4 id="unstructured-covariance-matrix">Unstructured covariance matrix<a class="anchor" aria-label="anchor" href="#unstructured-covariance-matrix"></a>
</h4>
<p>The most general model uses a saturated parametrization, i.e. any
covariance matrix could be represented in this form. Here we use
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mi>D</mi><mover><mi>L</mi><mo accent="true">̃</mo></mover></mrow><annotation encoding="application/x-tex">
L = D\tilde{L}
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
is the diagonal matrix of standard deviations, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>L</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{L}</annotation></semantics></math>
is a unit diagonal lower triangular matrix. Hence we start
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
with the natural logarithm of the standard deviations, followed by the
row-wise filled entries of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>L</mi><mo accent="true">̃</mo></mover><mo>=</mo><mo stretchy="false" form="prefix">{</mo><msub><mi>l</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mo stretchy="false" form="postfix">}</mo><mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>&lt;</mo><mi>i</mi><mo>≤</mo><mi>m</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\tilde{L} = \{l_{ij}\}_{1 \leq j &lt; i \leq m}</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mrow><mi>log</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>σ</mi><mn>1</mn></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo><mi>…</mi><mo>,</mo><mrow><mi>log</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>σ</mi><mi>m</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>,</mo><msub><mi>l</mi><mn>21</mn></msub><mo>,</mo><msub><mi>l</mi><mn>31</mn></msub><mo>,</mo><msub><mi>l</mi><mn>32</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>l</mi><mrow><mi>m</mi><mo>,</mo><mi>m</mi><mo>−</mo><mn>1</mn></mrow></msub><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">
\theta = (
  \log(\sigma_1), \dotsc, \log(\sigma_m),
  l_{21}, l_{31}, l_{32}, \dotsc, l_{m,m-1}
)^\top
</annotation></semantics></math> Here
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>m</mi><mo stretchy="false" form="prefix">(</mo><mi>m</mi><mo>+</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">k = m(m+1)/2</annotation></semantics></math>
entries. For example for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">m = 4</annotation></semantics></math>
time points we need
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">k = 10</annotation></semantics></math>
variance parameters to model the unstructured covariance matrix.</p>
<p>Other covariance matrix choices are explained in the <a href="covariance.html">covariance structures vignette</a>.</p>
</div>
<div class="section level4">
<h4 id="grouped-covariance-matrix">Grouped covariance matrix<a class="anchor" aria-label="anchor" href="#grouped-covariance-matrix"></a>
</h4>
<p>In some cases, we would like to estimate unique covariance matrices
across groups, while keeping the covariance structure (unstructured,
ante-dependence, Toeplitz, etc.) consistent across groups. Following the
notations in the previous section, for subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">g(i)</annotation></semantics></math>,
we have</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi mathvariant="normal">Σ</mi><mrow><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
\Sigma_{i} = S_i^\top \Sigma_{g(i)} S_i
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">g(i)</annotation></semantics></math>
is the group of subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi mathvariant="normal">Σ</mi><mrow><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo></mrow></msub><annotation encoding="application/x-tex">\Sigma_{g(i)}</annotation></semantics></math>
is the covariance matrix of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false" form="prefix">(</mo><mi>i</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">g(i)</annotation></semantics></math>.</p>
<p>The parametrization of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is similar to other non-grouped
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
Assume that there are total number of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>
groups, the length of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is multiplied by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>,
and for each part,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
is parametrized in the same fashion. For example, for an unstructured
covariance matrix,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>G</mi><mo>*</mo><mi>m</mi><mo stretchy="false" form="prefix">(</mo><mi>m</mi><mo>+</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">k = G * m(m+1)/2</annotation></semantics></math>
entries.</p>
</div>
<div class="section level4">
<h4 id="spatial-covariance-matrix">Spatial covariance matrix<a class="anchor" aria-label="anchor" href="#spatial-covariance-matrix"></a>
</h4>
<p>A spatial covariance structure can model individual-specific visit
times. An individual’s covariance matrix is then a function of both the
population-level covariance parameters (specific to the chosen
structure) and the individual’s visit times. Following the notations in
the previous section, for subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
with total number of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding="application/x-tex">m_i</annotation></semantics></math>
visits, we have</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub><mo>=</mo><mi>σ</mi><mo>*</mo><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false" form="prefix">(</mo><msub><mi>𝐜</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>𝐜</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
\sigma_{ijk} = \sigma * f(dist(\boldsymbol{c}_{ij}, \boldsymbol{c}_{ik}))
</annotation></semantics></math></p>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>m</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>m</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(m_{ij}, m_{ik})</annotation></semantics></math>
element of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\Sigma_{i}</annotation></semantics></math>
is a function of the distance between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">m_{ij}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><annotation encoding="application/x-tex">m_{ik}</annotation></semantics></math>
visit occurring on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><msub><mi>m</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msub><annotation encoding="application/x-tex">t_{m_{ij}}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><msub><mi>m</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub></msub><annotation encoding="application/x-tex">t_{m_{ik}}</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><msub><mi>m</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msub><annotation encoding="application/x-tex">t_{m_{ij}}</annotation></semantics></math>
is the coordinate(time) of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">m_{ij}</annotation></semantics></math>
visit for subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
is the constant variance. Usually we use Euclidean distance.</p>
<p>Currently only spatial exponential covariance structure is
implemented. For coordinates with multiple dimensions, the Euclidean
distance is used without transformations.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation<a class="anchor" aria-label="anchor" href="#maximum-likelihood-estimation"></a>
</h2>
<p>Given the general linear model above, and conditional on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
we know that the likelihood for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>β</mi><mo>;</mo><mi>Y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>π</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mi>N</mi><mi>/</mi><mn>2</mn></mrow></msup><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Ω</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mrow><mi>exp</mi><mo>⁡</mo></mrow><mrow><mo stretchy="true" form="prefix">{</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">}</mo></mrow></mrow><annotation encoding="application/x-tex">
L(\beta; Y) = (2\pi)^{-N/2} \det(\Omega)^{-1/2}
\exp\left\{
- \frac{1}{2}(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
\right\}
</annotation></semantics></math> and we also know that the maximum
likelihood (ML) estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
is the weighted least squares estimator
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\beta}</annotation></semantics></math>
solving the estimating equation
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>X</mi><mo stretchy="false" form="postfix">)</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
(X^\top \Omega^{-1} X) \hat{\beta} = X^\top \Omega^{-1} Y.
</annotation></semantics></math> Plugging in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\beta}</annotation></semantics></math>
into the likelihood above gives then the value of the function we want
to maximize with regards to the variance parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
Practically this will be done on the negative log scale:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>θ</mi><mo>;</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>−</mi><mrow><mi>log</mi><mo>⁡</mo></mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>;</mo><mi>Y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mi>N</mi><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mover><mi>β</mi><mo accent="true">̂</mo></mover><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
f(\theta; \hat{\beta}) = - \log L(\hat{\beta}; Y) = \frac{N}{2} \log(2\pi) +
  \frac{1}{2}\log\det(\Omega) +
  \frac{1}{2} (Y - X\hat{\beta})^\top \Omega^{-1} (Y - X\hat{\beta})
</annotation></semantics></math> The objective function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>θ</mi><mo>;</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(\theta; \hat{\beta})</annotation></semantics></math>
is then minimized with numerical optimizers utilizing
quasi-Newton-Raphson algorithms based on the gradient (or additionally
with Hessian, see <a href="introduction.html#optimizer">optimizer</a>).
Here the use of the Template Model Builder package <code>TMB</code> is
helpful because</p>
<ol style="list-style-type: decimal">
<li>
<code>TMB</code> allows to perform the calculations in C++, which
maximizes the speed.</li>
<li>
<code>TMB</code> performs automatic differentiation of the objective
function with regards to the variance parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>,
so that gradient and Hessian do not have to be approximated numerically
or coded explicitly.</li>
</ol>
<div class="section level3">
<h3 id="weighted-least-squares-estimator">Weighted least squares estimator<a class="anchor" aria-label="anchor" href="#weighted-least-squares-estimator"></a>
</h3>
<p>Let’s have a look at the details of calculating the log likelihood
above, including in particular the weighted least squares (WLS)
estimator
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\beta}</annotation></semantics></math>.</p>
<p>Starting point is the linear equation above and the observation that
both the left and right hand sides can be decomposed into
subject-specific terms given the block-diagonal structure of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>
and therefore its inverse,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">W = \Omega^{-1}</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>X</mi><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>W</mi><mi>X</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi>W</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
X^\top \Omega^{-1} X = X^\top W X =  \sum_{i=1}^{n} X_i^\top W_i X_i
</annotation></semantics></math> and similarly
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>W</mi><mi>Y</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi>W</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
X^\top \Omega^{-1} Y = X^\top W Y =  \sum_{i=1}^{n} X_i^\top W_i Y_i
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi mathvariant="normal">Σ</mi><mi>i</mi><mrow><mi>−</mi><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">W_i = \Sigma_i^{-1}</annotation></semantics></math>
is the weight matrix for subject
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
the inverse of its covariance matrix.</p>
<p>Instead of calculating this inverse explicitly, it is always better
numerically to work with the Cholesky factorization and solve linear
equations instead. Here we calculate the factorization
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><mo>=</mo><msub><mi>L</mi><mi>i</mi></msub><msubsup><mi>L</mi><mi>i</mi><mi>⊤</mi></msubsup></mrow><annotation encoding="application/x-tex">\Sigma_i = L_i L_i^\top</annotation></semantics></math>.
Note that in the case where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>=</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m_i = m</annotation></semantics></math>,
i.e. this subject has all time points observed, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><mo>=</mo><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma_i = \Sigma</annotation></semantics></math>
and we don’t need to calculate this again because we have already
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">\Sigma = L L^\top</annotation></semantics></math>,
i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">L_i = L</annotation></semantics></math>.
Unfortunately, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub><mo>&lt;</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m_i &lt; m</annotation></semantics></math>,
then we need to calculate this explicitly, as there is no way to update
the Cholesky factorization for a subset operation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>S</mi><mi>i</mi><mi>⊤</mi></msubsup><mi mathvariant="normal">Σ</mi><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Sigma_i = S_i^\top \Sigma S_i</annotation></semantics></math>
as we have above. Given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding="application/x-tex">L_i</annotation></semantics></math>,
we solve
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><msub><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
L_i \tilde{X}_i = X_i
</annotation></semantics></math> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\tilde{X}_i</annotation></semantics></math>
with an efficient forward-solve, and similarly we solve
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><msub><mover><mi>Y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>Y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
L_i \tilde{Y}_i = Y_i
</annotation></semantics></math> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>Y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\tilde{Y}_i</annotation></semantics></math>.
Therefore we have
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi>W</mi><mi>i</mi></msub><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><msubsup><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>i</mi><mi>⊤</mi></msubsup><msub><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
X_i^\top W_i X_i = \tilde{X}_i^\top \tilde{X}_i
</annotation></semantics></math> and
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>X</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi>W</mi><mi>i</mi></msub><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msubsup><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>i</mi><mi>⊤</mi></msubsup><msub><mover><mi>Y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
X_i^\top W_i Y_i = \tilde{X}_i^\top \tilde{Y}_i
</annotation></semantics></math> and we can thereby calculate the left
and right hand sides for the WLS estimating equation. We solve this
equation with a robust Cholesky decomposition with pivoting. The
advantage is that we can reuse this decomposition for calculating the
covariance matrix of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\beta}</annotation></semantics></math>,
i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>W</mi><mi>X</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">K = (X^\top W X)^{-1}</annotation></semantics></math>,
by supplying the identity matrix as alternative right hand side.</p>
</div>
<div class="section level3">
<h3 id="determinant-and-quadratic-form">Determinant and quadratic form<a class="anchor" aria-label="anchor" href="#determinant-and-quadratic-form"></a>
</h3>
<p>For the objective function we also need the log determinant of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">{</mo><mtext mathvariant="normal">blockdiag</mtext><msub><mi mathvariant="normal">Σ</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mi>n</mi></msub><mo stretchy="false" form="postfix">}</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>det</mi><mo>⁡</mo></mrow><msub><mi mathvariant="normal">Σ</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>log</mi><mo>⁡</mo></mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><mrow><msub><mi>L</mi><mi>i</mi></msub><msubsup><mi>L</mi><mi>i</mi><mi>⊤</mi></msubsup></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>log</mi><mo>⁡</mo></mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><msub><mi>L</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>m</mi><mi>i</mi></msub></munderover><mrow><mi>log</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><msub><mi>l</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mi>j</mi></mrow></msub><mo stretchy="false" form="postfix">)</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
\frac{1}{2}\log\det(\Omega)
  &amp;= \frac{1}{2}\log\det\{\text{blockdiag} \Sigma_1, \dotsc, \Sigma_n\} \\
  &amp;= \frac{1}{2}\log\prod_{i=1}^{n}\det{\Sigma_i} \\
  &amp;= \frac{1}{2}\sum_{i=1}^{n}\log\det{L_i L_i^\top} \\
  &amp;= \sum_{i=1}^{n}\log\det{L_i} \\
  &amp;= \sum_{i=1}^{n}\sum_{j=1}^{m_i}\log(l_{i, jj})
\end{align}</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>l</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">l_{i,jj}</annotation></semantics></math>
are the diagonal entries of the factor
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>L</mi><mi>i</mi></msub><annotation encoding="application/x-tex">L_i</annotation></semantics></math>
and we have used that</p>
<ul>
<li>the determinant of a block diagonal matrix is the product of the
determinants of the blocks,</li>
<li>the determinant of the product of matrices is the product of the
determinants,</li>
<li>the determinant of the transposed matrix is the same as the original
one,</li>
<li>the determinant of a triangular matrix is the product of the
diagonal.</li>
</ul>
<p>And finally, for the quadratic form we can reuse the weighted
response vector and design matrix:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mover><mi>β</mi><mo accent="true">̂</mo></mover><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>X</mi><mi>i</mi></msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msub><mi>W</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>Y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>X</mi><mi>i</mi></msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mo>−</mo><msub><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><mo stretchy="false" form="prefix">(</mo><msub><mover><mi>Y</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mo>−</mo><msub><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>i</mi></msub><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">
(Y - X\hat{\beta})^\top \Omega^{-1} (Y - X\hat{\beta}) =
\sum_{i=1}^{n} (Y_i - X_i\hat{\beta})^\top W_i (Y_i - X_i\hat{\beta}) =
\sum_{i=1}^{n} (\tilde{Y}_i - \tilde{X}_i\hat{\beta})^\top (\tilde{Y}_i - \tilde{X}_i\hat{\beta})
</annotation></semantics></math></p>
</div>
</div>
<div class="section level2">
<h2 id="restricted-maximum-likelihood-estimation">Restricted Maximum Likelihood Estimation<a class="anchor" aria-label="anchor" href="#restricted-maximum-likelihood-estimation"></a>
</h2>
<p>Under the restricted ML estimation (REML) paradigm we first obtain
the marginal likelihood of the variance parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
by integrating out the remaining parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
from the likelihood. Here we have:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>θ</mi><mo>;</mo><mi>Y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>p</mi></msup></msub><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>β</mi><mo>;</mo><mi>Y</mi><mo stretchy="false" form="postfix">)</mo><mi>d</mi><mi>β</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>π</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mi>N</mi><mi>/</mi><mn>2</mn></mrow></msup><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Ω</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>p</mi></msup></msub><mrow><mi>exp</mi><mo>⁡</mo></mrow><mrow><mo stretchy="true" form="prefix">{</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">}</mo></mrow><mi>d</mi><mi>β</mi></mrow><annotation encoding="application/x-tex">
L(\theta; Y) = \int_{\mathbb{R}^p} L(\beta; Y) d\beta =
(2\pi)^{-N/2} \det(\Omega)^{-1/2} \int_{\mathbb{R}^p}
\exp\left\{
- \frac{1}{2}(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
\right\}
d\beta
</annotation></semantics></math> where we note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\det(\Omega)</annotation></semantics></math>
depends on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
but not on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
and can therefore be pulled out of the integral.</p>
<div class="section level3">
<h3 id="completing-the-square">Completing the square<a class="anchor" aria-label="anchor" href="#completing-the-square"></a>
</h3>
<p>Let’s focus now on the quadratic form in the exponential function and
complete the square with regards to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
to obtain the kernel of a multivariate normal distribution:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><mo stretchy="false" form="postfix">)</mo></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>+</mo><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>X</mi><mi>β</mi><mo>−</mo><mn>2</mn><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>+</mo><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>β</mi><mo>−</mo><mn>2</mn><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>K</mi><msup><mi>X</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>+</mo><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>β</mi><mo>−</mo><mn>2</mn><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>+</mo><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>β</mi><mo>−</mo><mn>2</mn><msup><mi>β</mi><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>+</mo><msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>−</mo><msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right; padding-right: 0"></mtd><mtd columnalign="left" style="text-align: left; padding-left: 0"><mo>=</mo><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>−</mo><msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo>+</mo><mo stretchy="false" form="prefix">(</mo><mi>β</mi><mo>−</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>β</mi><mo>−</mo><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="false" form="postfix">)</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
&amp;= Y^\top \Omega^{-1} Y
+ \beta^\top X^\top \Omega^{-1} X \beta - 2 \beta^\top X^\top \Omega^{-1} Y \\
&amp;= Y^\top \Omega^{-1} Y + \beta^\top K^{-1} \beta
- 2 \beta^\top K^{-1}K X^\top \Omega^{-1} Y \\
&amp;= Y^\top \Omega^{-1} Y + \beta^\top K^{-1} \beta - 2 \beta^\top K^{-1} \hat{\beta} \\
&amp;= Y^\top \Omega^{-1} Y + \beta^\top K^{-1} \beta - 2 \beta^\top K^{-1} \hat{\beta}
+ \hat{\beta}^{-1} K^{-1} \hat{\beta} - \hat{\beta}^{-1} K^{-1} \hat{\beta} \\
&amp;= Y^\top \Omega^{-1} Y - \hat{\beta}^{-1} K^{-1} \hat{\beta} +
(\beta - \hat{\beta})^\top K^{-1} (\beta - \hat{\beta})
\end{align}</annotation></semantics></math> where we used
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false" form="prefix">(</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>W</mi><mi>X</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">K = (X^\top W X)^{-1}</annotation></semantics></math>
and could early on identify
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math>
as the covariance matrix of the kernel of the multivariate normal of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
and then later
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>β</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\hat{\beta}</annotation></semantics></math>
as the mean vector.</p>
<p>With this, we know that the integral of the multivariate normal
kernel is the inverse of the normalizing constants, and thus
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∫</mo><msup><mi>ℝ</mi><mi>p</mi></msup></msub><mrow><mi>exp</mi><mo>⁡</mo></mrow><mrow><mo stretchy="true" form="prefix">{</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><msup><mo stretchy="false" form="postfix">)</mo><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="prefix">(</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>β</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="true" form="postfix">}</mo></mrow><mi>d</mi><mi>β</mi><mo>=</mo><mrow><mi>exp</mi><mo>⁡</mo></mrow><mrow><mo stretchy="true" form="prefix">{</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mrow><mi>−</mi><mn>1</mn></mrow></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">}</mo></mrow><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>π</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>p</mi><mi>/</mi><mn>2</mn></mrow></msup><mrow><mi>det</mi><mo>⁡</mo></mrow><msup><mi>K</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">
\int_{\mathbb{R}^p}
\exp\left\{
- \frac{1}{2}(Y - X\beta)^\top \Omega^{-1} (Y - X\beta)
\right\}
d\beta =
\exp\left\{
-\frac{1}{2} Y^\top \Omega^{-1} Y + \frac{1}{2} \hat{\beta}^{-1} K^{-1} \hat{\beta}
\right\}
(2\pi)^{p/2} \det{K}^{1/2}
</annotation></semantics></math> such that the integrated likelihood is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>θ</mi><mo>;</mo><mi>Y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>π</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mo stretchy="false" form="prefix">(</mo><mi>N</mi><mo>−</mo><mi>p</mi><mo stretchy="false" form="postfix">)</mo><mi>/</mi><mn>2</mn></mrow></msup><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Ω</mi><msup><mo stretchy="false" form="postfix">)</mo><mrow><mi>−</mi><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mrow><mi>det</mi><mo>⁡</mo></mrow><msup><mi>K</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mrow><mi>exp</mi><mo>⁡</mo></mrow><mrow><mo stretchy="true" form="prefix">{</mo><mi>−</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>Y</mi><mi>⊤</mi></msup><msup><mi mathvariant="normal">Ω</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>Y</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>⊤</mi></msup><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">}</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">
L(\theta; Y) =
(2\pi)^{-(N-p)/2} \det(\Omega)^{-1/2} \det{K}^{1/2}
\exp\left\{
-\frac{1}{2} Y^\top \Omega^{-1} Y + \frac{1}{2} \hat{\beta}^\top K^{-1} \hat{\beta}
\right\}.
</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="objective-function">Objective function<a class="anchor" aria-label="anchor" href="#objective-function"></a>
</h3>
<p>As objective function which we want to minimize with regards to the
variance parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>
we again take the negative natural logarithm
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>θ</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>−</mi><mrow><mi>log</mi><mo>⁡</mo></mrow><mi>L</mi><mo stretchy="false" form="prefix">(</mo><mi>θ</mi><mo>;</mo><mi>Y</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mi>N</mi><mo>−</mo><mi>p</mi></mrow><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mn>2</mn><mi>π</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi mathvariant="normal">Ω</mi><mo stretchy="false" form="postfix">)</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>log</mi><mo>⁡</mo></mrow><mrow><mi>det</mi><mo>⁡</mo></mrow><mo stretchy="false" form="prefix">(</mo><mi>K</mi><mo stretchy="false" form="postfix">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mover><mi>Y</mi><mo accent="true">̃</mo></mover><mi>⊤</mi></msup><mover><mi>Y</mi><mo accent="true">̃</mo></mover><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mover><mi>β</mi><mo accent="true">̂</mo></mover><mi>⊤</mi></msup><msup><mover><mi>X</mi><mo accent="true">̃</mo></mover><mi>⊤</mi></msup><mover><mi>X</mi><mo accent="true">̃</mo></mover><mover><mi>β</mi><mo accent="true">̂</mo></mover></mrow><annotation encoding="application/x-tex">
f(\theta) = -\log L(\theta;Y) =
\frac{N-p}{2} \log(2\pi) + \frac{1}{2}\log\det(\Omega) - \frac{1}{2}\log\det(K)
+ \frac{1}{2} \tilde{Y}^\top \tilde{Y} - \frac{1}{2} \hat{\beta}^\top \tilde{X}^\top \tilde{X} \hat{\beta}
</annotation></semantics></math> It is interesting to see that
computation of the REML objective function is only requiring a few
additional calculations compared to the ML objective function. In
particular, since we already have the matrix decomposition of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>K</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">K^{-1}</annotation></semantics></math>,
it is very easy to obtain the determinant of it.</p>
<p>Also here we use numeric optimization of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false" form="prefix">(</mo><mi>θ</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">f(\theta)</annotation></semantics></math>
and the <code>TMB</code> library supports this efficiently through
automatic differentiation.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Daniel Sabanes Bove, Liming Li, Julia Dedic, Doug Kelkhoff, Kevin Kunzmann, Brian Matthew Lang, Christian Stock, Ya Wang, Dan James, Jonathan Sidi, Daniel Leibovitz, Daniel D. Sjoberg, Nikolas Ivan Krieger, Boehringer Ingelheim Ltd., Gilead Sciences, Inc., F. Hoffmann-La Roche AG, Merck Sharp &amp; Dohme, Inc., AstraZeneca plc, inferential.biostatistics GmbH.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
