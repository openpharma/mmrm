<!DOCTYPE html>
<!-- Generated by pkgdown + https://github.com/insightsengineering/r-pkgdown-multiversion -->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Satterthwaite • mmrm</title>
<!-- mathjax math --><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script><script>
  window.MathJax = {
    chtml: {
      fontURL: "https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/output/chtml/fonts/woff-v2"
    }
  };
</script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Satterthwaite">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-125641273-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125641273-1');
</script>
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">mmrm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.16</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Introduction</h6></li>
    <li><a class="dropdown-item" href="../articles/introduction.html">Package Introduction</a></li>
    <li><a class="dropdown-item" href="../articles/methodological_introduction.html">Mixed Models for Repeated Measures</a></li>
    <li><a class="dropdown-item" href="../articles/mmrm_review_methods.html">Comparison with other software</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Covariance Structures</h6></li>
    <li><a class="dropdown-item" href="../articles/covariance.html">Covariance Structures</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Degrees of Freedom and Testing</h6></li>
    <li><a class="dropdown-item" href="../articles/hypothesis_testing.html">Details of Hypothesis Testing</a></li>
    <li><a class="dropdown-item" href="../articles/between_within.html">Between-Within</a></li>
    <li><a class="dropdown-item" href="../articles/kenward.html">Kenward-Roger</a></li>
    <li><a class="dropdown-item" href="../articles/satterthwaite.html">Satterthwaite</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Coefficient Covariance</h6></li>
    <li><a class="dropdown-item" href="../articles/coef_vcov.html">Coefficients Covariance Matrix Adjustment</a></li>
    <li><a class="dropdown-item" href="../articles/empirical_wls.html">Details of Weighted Least Square Empirical Covariance</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Algorithm Details</h6></li>
    <li><a class="dropdown-item" href="../articles/algorithm.html">Model Fitting Algorithm</a></li>
    <li><a class="dropdown-item" href="../articles/predict.html">Prediction and Simulation</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Developers</h6></li>
    <li><a class="dropdown-item" href="../articles/package_structure.html">Package Structure</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-reports" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Reports</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-reports">
<li><a class="dropdown-item" href="../coverage-report/">Coverage report</a></li>
    <li><a class="dropdown-item" href="../unit-test-report/">Unit test report</a></li>
  </ul>
</li>
      <div><li class="nav-item dropdown">
    <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-versions">Versions</a>
    <div class="dropdown-menu" aria-labelledby="dropdown-versions">
    <a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/main">main</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/latest-tag">latest-tag</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.16">v0.3.16</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.15">v0.3.15</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.14">v0.3.14</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.13">v0.3.13</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.12">v0.3.12</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.11">v0.3.11</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.10">v0.3.10</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.7">v0.3.7</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.6">v0.3.6</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.5">v0.3.5</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.4">v0.3.4</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.3">v0.3.3</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.2">v0.3.2</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.1">v0.3.1</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.3.0">v0.3.0</a>
<a class="dropdown-item" data-toggle="tooltip" title="" href="https://openpharma.github.io/mmrm/v0.2.2">v0.2.2</a>
</div>
</li></div>
</ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/openpharma/mmrm"><span class="fa fa-github"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Satterthwaite</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/openpharma/mmrm/blob/v0.3.16/vignettes/satterthwaite.Rmd" class="external-link"><code>vignettes/satterthwaite.Rmd</code></a></small>
      <div class="d-none name"><code>satterthwaite.Rmd</code></div>
    </div>

    
    
<p>Here we describe the details of the Satterthwaite degrees of freedom
calculations.</p>
<div class="section level3">
<h3 id="satterthwaite-degrees-of-freedom-for-asymptotic-covariance">Satterthwaite degrees of freedom for asymptotic covariance<a class="anchor" aria-label="anchor" href="#satterthwaite-degrees-of-freedom-for-asymptotic-covariance"></a>
</h3>
<p>In <span class="citation">Christensen (2018)</span> the Satterthwaite
degrees of freedom approximation based on normal models is well detailed
and the computational approach for models fitted with the
<code>lme4</code> package is explained. We follow the algorithm and
explain the implementation in this <code>mmrm</code> package. The model
definition is the same as in <a href="algorithm.html">Details of the
model fitting in <code>mmrm</code></a>.</p>
<p>We are also using the same notation as in the <a href="kenward.html">Details of the Kenward-Roger calculations</a>. In
particular, we assume we have a contrast matrix <span class="math inline">\(C \in \mathbb{R}^{c\times p}\)</span> with which
we want to test the linear hypothesis <span class="math inline">\(C\beta
= 0\)</span>. Further, <span class="math inline">\(W(\hat\theta)\)</span> is the inverse of the
Hessian matrix of the log-likelihood function of <span class="math inline">\(\theta\)</span> evaluated at the estimate <span class="math inline">\(\hat\theta\)</span>, i.e. the observed Fisher
Information matrix as a consistent estimator of the variance-covariance
matrix of <span class="math inline">\(\hat\theta\)</span>. <span class="math inline">\(\Phi(\theta) = \left\{X^\top \Omega(\theta)^{-1}
X\right\} ^{-1}\)</span> is the asymptotic covariance matrix of <span class="math inline">\(\hat\beta\)</span>.</p>
<div class="section level4">
<h4 id="one-dimensional-contrast">One-dimensional contrast<a class="anchor" aria-label="anchor" href="#one-dimensional-contrast"></a>
</h4>
<p>We start with the case of a one-dimensional contrast, i.e. <span class="math inline">\(c = 1\)</span>. The Satterthwaite adjusted degrees
of freedom for the corresponding t-test are then defined as: <span class="math display">\[
\hat\nu(\hat\theta) = \frac{2f(\hat\theta)^2}{f{'}(\hat\theta)^\top
W(\hat\theta) f{'}(\hat\theta)}
\]</span> where <span class="math inline">\(f(\hat\theta) = C
\Phi(\hat\theta) C^\top\)</span> is the scalar in the numerator and we
can identify it as the variance estimate for the estimated scalar
contrast <span class="math inline">\(C\hat\beta\)</span>. The
computational challenge is essentially to evaluate the denominator in
the expression for <span class="math inline">\(\hat\nu(\hat\theta)\)</span>, which amounts to
computing the <span class="math inline">\(k\)</span>-dimensional
gradient <span class="math inline">\(f{'}(\hat\theta)\)</span> of
<span class="math inline">\(f(\theta)\)</span> (for the given contrast
matrix <span class="math inline">\(C\)</span>) at the estimate <span class="math inline">\(\hat\theta\)</span>. We already have the
variance-covariance matrix <span class="math inline">\(W(\hat\theta)\)</span> of the variance parameter
vector <span class="math inline">\(\theta\)</span> from the model
fitting.</p>
<div class="section level5">
<h5 id="jacobian-approach">Jacobian approach<a class="anchor" aria-label="anchor" href="#jacobian-approach"></a>
</h5>
<p>However, if we proceeded in a naive way here, we would need to
recompute the denominator again for every chosen <span class="math inline">\(C\)</span>. This would be slow, e.g. when changing
<span class="math inline">\(C\)</span> every time we want to test a
single coefficient within <span class="math inline">\(\beta\)</span>. It
is better to instead evaluate the gradient of the matrix valued function
<span class="math inline">\(\Phi(\theta)\)</span>, which is therefore
the Jacobian, with regards to <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\mathcal{J}(\theta) = \nabla_\theta
\Phi(\theta)\)</span>. Imagine <span class="math inline">\(\mathcal{J}(\theta)\)</span> as the the
3-dimensional array with <span class="math inline">\(k\)</span> faces of
size <span class="math inline">\(p\times p\)</span>. Left and right
multiplying each face by <span class="math inline">\(C\)</span> and
<span class="math inline">\(C^\top\)</span> respectively leads to the
<span class="math inline">\(k\)</span>-dimensional gradient <span class="math inline">\(f'(\theta) = C \mathcal{J}(\theta)
C^\top\)</span>. Therefore for each new contrast <span class="math inline">\(C\)</span> we just need to perform simple matrix
multiplications, which is fast (see <code><a href="../reference/h_gradient.html">h_gradient()</a></code> where this
is implemented). Thus, having computed the estimated Jacobian <span class="math inline">\(\mathcal{J}(\hat\theta)\)</span>, it is only a
matter of putting the different quantities together to compute the
estimate of the denominator degrees of freedom, <span class="math inline">\(\hat\nu(\hat\theta)\)</span>.</p>
</div>
<div class="section level5">
<h5 id="jacobian-calculation">Jacobian calculation<a class="anchor" aria-label="anchor" href="#jacobian-calculation"></a>
</h5>
<p>Currently, we evaluate the gradient of <span class="math inline">\(\Phi(\theta)\)</span> through function
<code><a href="../reference/h_jac_list.html">h_jac_list()</a></code>. It uses automatic differentiation provided in
<code>TMB</code>.</p>
<p>We first obtain the Jacobian of the inverse of the covariance matrix
of coefficient (<span class="math inline">\(\Phi(\theta)^{-1}\)</span>),
following the <a href="kenward.html#special-considerations-for-mmrm-models">Kenward-Roger
calculations</a>. Please note that we only need <span class="math inline">\(P_h\)</span> matrices.</p>
<p>Then, to obtain the Jacobian of the covariance matrix of coefficient,
following the <a href="kenward.html#derivative-of-the-sigma-1">algorithm</a>, we use
<span class="math inline">\(\Phi(\theta)\)</span> estimated in the fit
to obtain the Jacobian.</p>
<p>The result is a list (of length <span class="math inline">\(k\)</span> where <span class="math inline">\(k\)</span> is the dimension of the variance
parameter <span class="math inline">\(\theta\)</span>) of matrices of
<span class="math inline">\(p \times p\)</span>, where <span class="math inline">\(p\)</span> is the dimension of <span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
<div class="section level4">
<h4 id="multi-dimensional-contrast">Multi-dimensional contrast<a class="anchor" aria-label="anchor" href="#multi-dimensional-contrast"></a>
</h4>
<p>When <span class="math inline">\(c &gt; 1\)</span> we are testing
multiple contrasts at once. Here an F-statistic <span class="math display">\[
F = \frac{1}{c} (C\hat\beta)^\top  (C \Phi(\hat\theta) C^\top)^{-1}
C^\top (C\hat\beta)
\]</span> is calculated, and we are interested in estimating an
appropriate denominator degrees of freedom for <span class="math inline">\(F\)</span>, while assuming <span class="math inline">\(c\)</span> are the numerator degrees of freedom.
Note that only in special cases, such as orthogonal or balanced designs,
the F distribution will be exact under the null hypothesis. In general,
it is an approximation.</p>
<p>The calculations are described in detail in <span class="citation">Christensen (2018)</span>, and we don’t repeat them
here in detail. The implementation is in <code><a href="../reference/h_df_md_sat.html">h_df_md_sat()</a></code> and
starts with an eigen-decomposition of the asymptotic variance-covariance
matrix of the contrast estimate, i.e. <span class="math inline">\(C
\Phi(\hat\theta) C^\top\)</span>. The F-statistic can be rewritten as a
sum of <span class="math inline">\(t^2\)</span> statistics based on
these eigen-values. The corresponding random variables are independent
(by design because they are derived from the orthogonal eigen-vectors)
and essentially have one degree of freedom each. Hence, each of the
<span class="math inline">\(t\)</span> statistics is treated as above in
the one-dimensional contrast case, i.e. the denominator degree of
freedom is calculated for each of them. Finally, using properties of the
F distribution’s expectation, the denominator degree of freedom for the
whole F statistic is derived.</p>
</div>
</div>
<div class="section level3">
<h3 id="satterthwaite-degrees-of-freedom-for-empirical-covariance">Satterthwaite degrees of freedom for empirical covariance<a class="anchor" aria-label="anchor" href="#satterthwaite-degrees-of-freedom-for-empirical-covariance"></a>
</h3>
<p>In <span class="citation">Bell and McCaffrey (2002)</span> the
Satterthwaite degrees of freedom in combination with a sandwich
covariance matrix estimator are described.</p>
<div class="section level4">
<h4 id="one-dimensional-contrast-1">One-dimensional contrast<a class="anchor" aria-label="anchor" href="#one-dimensional-contrast-1"></a>
</h4>
<p>For one-dimensional contrast, following the same notation in <a href="algorithm.html">Details of the model fitting in
<code>mmrm</code></a> and <a href="kenward.html">Details of the
Kenward-Roger calculations</a>, we have the following derivation. For an
estimator of variance with the following term</p>
<p><span class="math display">\[
  v = s c^\top(X^\top X)^{-1}\sum_{i}{X_i^\top A_i \epsilon_i
\epsilon_i^\top A_i X_i} (X^\top X)^{-1} c
\]</span></p>
<p>where <span class="math inline">\(s\)</span> takes the value of <span class="math inline">\(\frac{n}{n-1}\)</span>, <span class="math inline">\(1\)</span> or <span class="math inline">\(\frac{n-1}{n}\)</span>, and <span class="math inline">\(A_i\)</span> takes <span class="math inline">\(I_i\)</span>, <span class="math inline">\((I_i -
H_{ii})^{-\frac{1}{2}}\)</span>, or <span class="math inline">\((I_i -
H_{ii})^{-1}\)</span> respectively, <span class="math inline">\(c\)</span> is a column vector, then <span class="math inline">\(v\)</span> can be decomposed into the a weighted
sum of independent <span class="math inline">\(\chi_1^2\)</span>
distribution, where the weights are the eigenvalues of the <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(G\)</span> with elements <span class="math display">\[
  G_{ij} = g_i^\top V g_j
\]</span></p>
<p>where</p>
<p><span class="math display">\[
  g_i = s^{\frac{1}{2}} (I - H)_i^\top A_i X_i (X^\top X)^{-1} c
\]</span> <span class="math display">\[
  H = X(X^\top X)^{-1}X^\top
\]</span></p>
<p><span class="math inline">\((I - H)_i\)</span> corresponds to the
rows of subject <span class="math inline">\(i\)</span>.</p>
<p>So the degrees of freedom can be represented as <span class="math display">\[
  \nu = \frac{(\sum_{i}\lambda_i)^2}{\sum_{i}{\lambda_i^2}}
\]</span></p>
<p>where <span class="math inline">\(\lambda_i, i = 1, \dotsc,
n\)</span> are the eigenvalues of <span class="math inline">\(G\)</span>. <span class="citation">Bell and
McCaffrey (2002)</span> also suggests that <span class="math inline">\(V\)</span> can be chosen as identify matrix, so
<span class="math inline">\(G_{ij} = g_i ^\top g_j\)</span>.</p>
<p>Following <a href="algorithm.html#weighted-least-squares-estimator">Weighted Least
Square Estimator</a>, we can transform the original <span class="math inline">\(X\)</span> into <span class="math inline">\(\tilde{x}\)</span> to use the above equations.</p>
<p>To avoid repeated computation of matrix <span class="math inline">\(A_i\)</span>, <span class="math inline">\(H\)</span> etc for different contrasts, we
calculate and cache the following</p>
<p><span class="math display">\[
  G^\ast_i = (I - H)_i^\top A_i X_i (X^\top X)^{-1}
\]</span> which is a <span class="math inline">\(\sum_i{m_i} \times
p\)</span> matrix. With different contrasts, we need only calculate the
following <span class="math display">\[
  g_i = G^\ast_i c
\]</span> to obtain a <span class="math inline">\(\sum_i{m_i} \times
1\)</span> matrix, <span class="math inline">\(G\)</span> can be
computed with <span class="math inline">\(g_i\)</span>.</p>
<p>To obtain the degrees of freedom, and to avoid eigen computation on a
large matrix, we can use the following equation</p>
<p><span class="math display">\[
  \nu = \frac{(\sum_{i}\lambda_i)^2}{\sum_{i}{\lambda_i^2}} =
\frac{tr(G)^2}{\sum_{i}{\sum_{j}{G_{ij}^2}}}
\]</span></p>
<p>The scale parameter is not used throughout the package.</p>
<p>The proof is as following</p>
<ol style="list-style-type: decimal">
<li>Proof of <span class="math display">\[
  tr(AB) = tr(BA)
\]</span>
</li>
</ol>
<p>Let <span class="math inline">\(A\)</span> has dimension <span class="math inline">\(p\times q\)</span>, <span class="math inline">\(B\)</span> has dimension <span class="math inline">\(q\times p\)</span> <span class="math display">\[
  tr(AB) = \sum_{i=1}^{p}{(AB)_{ii}} =
\sum_{i=1}^{p}{\sum_{j=1}^{q}{A_{ij}B_{ji}}}
\]</span></p>
<p><span class="math display">\[
  tr(BA) = \sum_{i=1}^{q}{(BA)_{ii}} =
\sum_{i=1}^{q}{\sum_{j=1}^{p}{B_{ij}A_{ji}}}
\]</span></p>
<p>so <span class="math inline">\(tr(AB) = tr(BA)\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Proof of <span class="math display">\[
  tr(G) = \sum_{i}(\lambda_i)
\]</span> and <span class="math display">\[
  \sum_{i}(\lambda_i^2) = \sum_{i}{\sum_{j}{G_{ij}^2}}
\]</span> if <span class="math inline">\(G = G^\top\)</span>
</li>
</ol>
<p>Following eigen decomposition, we have <span class="math display">\[
  G = Q \Lambda Q^\top
\]</span> where <span class="math inline">\(\Lambda\)</span> is diagonal
matrix, <span class="math inline">\(Q\)</span> is orthogonal matrix.</p>
<p>Using the previous formula that <span class="math inline">\(tr(AB) =
tr(BA)\)</span>, we have</p>
<p><span class="math display">\[
  tr(G) = tr(Q \Lambda Q^\top) = tr(\Lambda Q^\top Q) = tr(\Lambda) =
\sum_{i}(\lambda_i)
\]</span></p>
<p><span class="math display">\[
  tr(G^\top G) = tr(Q \Lambda Q^\top Q \Lambda Q^\top) = tr(\Lambda^2
Q^\top Q) = tr(\Lambda^2) = \sum_{i}(\lambda_i^2)
\]</span></p>
<p>and <span class="math inline">\(tr(G^\top G)\)</span> can be further
expressed as</p>
<p><span class="math display">\[
  tr(G^\top G) = \sum_{i}{(G^\top G)_{ii}} =
\sum_{i}{\sum_{j}{G^\top_{ij}G_{ji}}} = \sum_{i}{\sum_{j}{G_{ij}^2}}
\]</span></p>
</div>
<div class="section level4">
<h4 id="multi-dimensional-contrast-1">Multi-dimensional contrast<a class="anchor" aria-label="anchor" href="#multi-dimensional-contrast-1"></a>
</h4>
<p>For multi-dimensional contrast we use the same technique for
multi-dimensional contrast for asymptotic covariance.</p>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-bell2002bias" class="csl-entry">
Bell RM, McCaffrey DF (2002). <span>“Bias Reduction in Standard Errors
for Linear Regression with Multi-Stage Samples.”</span> <em>Survey
Methodology</em>, <strong>28</strong>(2), 169–182.
</div>
<div id="ref-Christensen2018" class="csl-entry">
Christensen RHB (2018). <em>Satterthwaite’s Method for Degrees of
Freedom in Linear Mixed Models</em>. Retrieved from <a href="https://github.com/runehaubo/lmerTestR/blob/35dc5885205d709cdc395b369b08ca2b7273cb78/pkg_notes/Satterthwaite_for_LMMs.pdf" class="external-link">https://github.com/runehaubo/lmerTestR/blob/35dc5885205d709cdc395b369b08ca2b7273cb78/pkg_notes/Satterthwaite_for_LMMs.pdf</a>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Daniel Sabanes Bove, Liming Li, Julia Dedic, Doug Kelkhoff, Kevin Kunzmann, Brian Matthew Lang, Christian Stock, Ya Wang, Dan James, Jonathan Sidi, Daniel Leibovitz, Daniel D. Sjoberg, Nikolas Ivan Krieger, Boehringer Ingelheim Ltd., Gilead Sciences, Inc., F. Hoffmann-La Roche AG, Merck Sharp &amp; Dohme, Inc., AstraZeneca plc, inferential.biostatistics GmbH.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
