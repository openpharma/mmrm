---
title: "Idea for better optimization with unstructured covariance matrix"
author: "Daniel Sabanes Bove"
output: html_document
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective

The objective is to improve the optimization of variance parameters $\theta$ when using
an unstructured covariance matrix. Currently we just initialize all $\theta$ elements with
zero and then follow the optimizer algorithm, cf. `h_mmrm_tmb_parameters()`.

# Packages review

- How is `nlme::gls` doing this?
  - Seems this is happening in [`nlme:::Initialize.corSymm()`](https://github.com/cran/nlme/blob/master/R/corStruct.R#L440)
- How is `glmmTMB::glmmTMB` doing this?
  - They have a more clever approach, see [`glmmTMB:::startParams()`](https://github.com/glmmTMB/glmmTMB/blob/master/glmmTMB/R/glmmTMB.R#L37)
- How is `SAS PROC MIXED` doing this?
  - See [STARTING VALUES FOR PROC MIXED WITH REPEATED MEASURES DATA](https://newprairiepress.org/cgi/viewcontent.cgi?article=1269&context=agstatconference)
  - The default in `PROC MIXED` uses `MIVQUEO`. 
    - See the [SAS PROC MIXED manual](https://support.sas.com/documentation/onlinedoc/stat/141/mixed.pdf), p. 6132: "You can use the noniterative MIVQUE0 method to estimate G and R (Rao 1972; LaMotte 1973; Wolfinger, Tobias, and Sall 1994). In fact, by default PROC MIXED uses MIVQUE0 estimates as starting values for the ML and REML procedures."
    - References:
      - Wolfinger, R. D., Tobias, R. D., and Sall, J. (1994). “Computing Gaussian Likelihoods and Their Derivatives for General Linear Mixed Models.” SIAM Journal on Scientific Computing 15:1294–1310.
      - SAS Technical Report R-105, Computing MIVQUE0 Estimates of Variance Components. https://support.sas.com/documentation/onlinedoc/v82/techreport_r105.pdf
      - Giesbrecht, F. G. (1989). A General Structure for the Class of Mixed Linear Models. Southern Cooperative Series Bulletin 343, Louisiana Agricultural Experiment Station, Baton Rouge.
  - Another option is `OLS`, which sets starting values of all variances at 1 and all
covariances at O. (Note: This corresponds to our choice.)
  - The third option is to use a `PARMS` statement to enter one's "best guess". These maybe
obtained from previous closely related experiments or some other method.
  
# Example

Let's first generate a difficult data set.
This is taken from https://github.com/openpharma/mmrm/issues/380.

```{r}
library(MASS)
library(mvtnorm)
gen.FEV1 = function(
    N=1,
    n=200,
    mu=-100/52,
    delta = 50/52 ,
    mua = 2000,
    sigmaa = 300,
    sigmab = 60,
    corab = 0.2,
    sigma = 10,
    times = c(0,2,6,12,24,36,52,70,88,104)
){
  nt = length(times)                    # no of repeated measures
  out = as.data.frame(list(1,1,"Tmp",1,1))
  names(out) = c("Sim","Pts","Trt","Time","FEV1")                 # out is the output

  outi = as.data.frame(list(rep(0,n*nt),rep(0,n*nt),rep("Trt",n*nt),rep(0,n*nt),rep(0,n*nt)))
  names(outi) = names(out)                                        # outi is the output for each replication

  outi[,"Pts"] = rep(1:n,each=nt)
  outi[,"Trt"] = c(rep("Placebo",n*nt/2),rep("Active",n*nt/2))
  outi[,"Time"] = rep(times,n)                                   # the time of each FEV1 measurement
  covab = corab * sigmaa * sigmab                                # cov between a and b
  COV = matrix(c(sigmaa^2,covab,covab,sigmab^2),ncol=2)          # Cov matrix for the slope and intercept

  Pts.ID = 1:n
  Trt = c(rep("Placebo",n/2),rep("Active",n/2))

  for (i in 1:N)                                                 # Loop over the N simulated studies
  {outi[,"Sim"] = rep(i,n*nt)
  for (j in 1:n)                                            # Loop over the n patients
  {si = rmvnorm(1, mean = c(mua, mu + delta*(Trt[j]=="Active")),sigma=COV)
  outi[((j-1)*nt+1):(j*nt),"FEV1"] = si[1] + si[2] *times + rnorm(nt,0,sd=sigma)
  }
  out = rbind(out,outi)                                      # Append study to the full data
  }

  out = out[-1,]                                                # remove temporary row
  return(out)
}
set.seed(123)
out <- gen.FEV1()
out = cbind(out,paste("P",out[,"Pts"],sep=""),paste("T",out[,"Time"],sep=""))
names(out)[6:7] = c("Patient","Visit")
out[,"Visit"] = factor(out[,"Visit"], levels = paste0("T", sort(unique(out[, "Time"]))))
out[,"Patient"] = factor(out[,"Patient"])
out[,"Trt"] = factor(out[,"Trt"])

dat <- out[out[,"Sim"]==1,]
head(dat)
summary(dat)
```

Now let's try to fit this.

## `mmrm`

```{r}
result_mmrm1 <- fit_mmrm(
  FEV1 ~ Trt * Visit + us(Visit | Patient), data = dat, weights = rep(1, nrow(dat)),
  control = mmrm_control(optimizer = "BFGS", optimizer_control = list(maxit = 50000))
)
result_mmrm1
```

So this works but we see a convergence problem at the end (singular variance parameters Hessian).

```{r}
result_mmrm2 <- fit_mmrm(
  FEV1 ~ Trt * Visit + us(Visit | Patient), data = dat, weights = rep(1, nrow(dat)),
  control = mmrm_control(optimizer = "nlminb", optimizer_control = list(eval.max = 1000, iter.max = 1000))
)
result_mmrm2
```

So this takes even longer to run, but we get a better result.
At least the two are consistent with regards to the coefficient estimates:

```{r}
max(abs(coef(result_mmrm1) - coef(result_mmrm2)))
```

But we see a problem with estimating the variance parameters, with the largest differences being:

```{r}
tail(sort(component(result_mmrm1, "theta_est") - component(result_mmrm2, "theta_est")))
```

And we can also see this in very different covariance matrix estimates.
Let's try also L-BFGS-B:

```{r}
result_mmrm3 <- fit_mmrm(
  FEV1 ~ Trt * Visit + us(Visit | Patient), data = dat, weights = rep(1, nrow(dat)),
  control = mmrm_control(optimizer = "L-BFGS-B", optimizer_control = list(maxit = 50000))
)
result_mmrm3
```

Similar picture here, the coefficient estimates agree but the variance parameters are far apart:

```{r}
max(abs(coef(result_mmrm2) - coef(result_mmrm3)))
tail(sort(component(result_mmrm1, "theta_est") - component(result_mmrm3, "theta_est")))
tail(sort(component(result_mmrm2, "theta_est") - component(result_mmrm3, "theta_est")))
```

## `SAS`

```{r}
library(sasr.roche)
sas_code <- "PROC MIXED DATA = dat cl method=reml ols;
      CLASS Trt(ref = 'Active') Visit(ref = 'T0') Patient;
      MODEL FEV1 = Trt Visit Trt*Visit / ddfm=Satterthwaite solution;
      REPEATED Visit / subject=Patient type=un r rcorr;
    RUN;"
df2sd(dat, table = "dat")
sas_result <- run_sas(sas_code)
cat(sas_result$LST)
```

Only needs 1 iteration (!) and the deviance is 19131 and therefore close to what the
`nlminb` optimizer gave us. Also looking at the covariance matrix it seems very close.

So it *is* possible to fit this model on this data set!

Very interestingly, when using the `ols` option to use the same as our starting values,
it takes much longer to converge. So it seems the `mivque0` starting values are really
the decisive difference in the `PROC MIXED` algorithm performance in this example.

## `nlme` 

With `optim` optimizer this fails. 

```{r, eval = FALSE}
library(nlme)
result_gls <- gls(
  model = FEV1 ~ Trt * Visit,
  data = dat,
  correlation = corSymm(form = ~1|Patient),
  weights = varIdent(form = ~1|Visit),
  control = glsControl(opt = "nlminb", maxIter = 10000, msMaxIter = 10000, msVerbose = TRUE),
  method = "REML",
  na.action = "na.omit"
)
```

Hitting `function evaluation limit reached without convergence` but cannot control this.
So this does not work.

## `glmmTMB`

Also this fails.

```{r, eval = FALSE}
library(glmmTMB)
result_glmmtmb <- glmmTMB(
  FEV1 ~ Trt * Visit + us(0 + Visit | Patient),
  data = dat,
  dispformula = ~0,
  REML = TRUE
)
```

# Idea

1. First fit a standard linear model, obtain $\hat{\beta}$ and resulting residuals $\hat{\epsilon}$
1. Calculate the empirical covariance matrix $\hat{\Sigma}_{\text{emp}}$ of $\hat{\epsilon}$ as a very rough first initial guess for $\Sigma$
1. Derive corresponding variance parameters $\hat{\theta}_{\text{emp}}$
1. Initialize the optimization of $\theta$ with $\hat{\theta}_{\text{emp}}$

# Experiment

# Further ideas

- Is it worth to precondition the estimation, e.g. by dividing both $Y$ and corresponding $X$ rows with the visit-specific standard deviation of $\hat{\epsilon}$? And then rescale results back at the end? 

