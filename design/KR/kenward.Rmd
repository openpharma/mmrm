---
title: how to implement Kenward Roger degree of freedom?
author: "Liming Li"
output: bookdown::html_document2
editor_options:
  chunk_output_type: console
---

# Objective

The objective is to design how we can implement Kenward-Roger degree of freedom.

# Current design of mmrm package

In current mmrm package, we provided Satterthwaite degree of freedom, through `df_1d` and `df_md`.

# Design of Kenward-Roger degree of freedom

## update to current functions

* `df_1d` and `df_md` renamed to `df_1d_sat` and `df_md_sat`, do not export them
* add a new function `df_1d` and `df_md` to include degree of freedom selection
* implement `df_1d_kr` and `df_md_kr` (take extra arguments to conduct improved Kenward-Roger degree of freedom)

## Further updates to the naming
* consider renaming the `df_1d` and `df_md` to something like `beta_test_1d` and `beta_test_md`?(the names need to be discussed)
* add deprecation notes on `df_1d` and `df_md`

## pseudo code

```{r, eval=FALSE}
# df_1d
formula <- FEV1 ~ ARMCD + ad(AVISIT | USUBJID)
result <- mmrm(formula, fev_data, reml = FALSE)
df_1d(result, contrast = c(0, 1), df = "SAT") # test the armcd using SAT
# df_1d_sat is called
df_1d(result, contrast = c(0, 1), df = "KR") # KR defaults to KR2 (improved version)
# df_1d_kr2 is called
# give errors because KR works only for REML!

formula <- FEV1 ~ ARMCD + ad(AVISIT | USUBJID)
result <- mmrm(formula, fev_data, reml = TRUE)

df_1d(result, contrast = c(0, 1), df = "KR1")
# provide what we have in current `df_1d` results
```

## Rationals

Why we implement the Kenward-Roger df in `mmrm` instead of using the `pbkrtest` package directly?

* Current `mmrm` implementation is kind of complete, with the tests already implemented. If we are going to rely on `pbkrtest`
package to implement Kenward-Roger degree of freedom, we are not able to include this method in `mmrm` otherwise we can introduce
cyclic imports
* `pbkrtest` is not well tested and we need effort to reliablely use that package
* `pbkrtest` implements the original Kenward-Roger degree of freedom instead of the improved one(recommended)
* SAS software provides both Kenward-Roger degree of freedom
* `lmerTestR` suggests that "The culprit is the calculation of the scaling of the $F$-value for which **pbkrtest** does not appear to export a low-level (direct) method."
which leads to slow speed which we can improve

# Prototypes

## mathmatical details for KR1

### adjusted variance-covariance matrix

Following the vignettes' notations, we have
\[
  Y \sim N(X\beta, \Sigma)
\]
where $\Sigma$ is a block diagonal matrix.
Let
\[
  \Phi(\sigma) = {\left\{X^\top \Sigma(\sigma)^\top X\right\}}^{-1}
\]
\[
  \hat\beta = \Phi(\hat\sigma) X^\top \Sigma(\hat\sigma)^{-1} Y
\]

$\Phi$ is the variance-covariance matrix for $\hat\beta$, and we usually use
$\hat\Phi = \Phi(\hat\sigma)$ to measure the precision of $\hat\beta$.

However, Kackar and Harville shows that the variability can be further partitioned into
\begin{equation}
  V(\hat\beta) = \Phi + \Lambda (\#eq:vbeta)
\end{equation}
where $\Lambda$ represents the asymptotic variance-covariance matrix underestimates $V(\hat\beta)$,
and $\Lambda$ can be approximated by
\begin{equation}
  \Lambda \simeq \Phi \left\{\sum_{i=1}^r{\sum_{j=1}^r{W_{ij}(Q_{ij} - P_i \Phi P_j)} }\right\} \Phi (\#eq:lambda)
\end{equation}
where
\[
  P_i = X^\top \frac{\partial{\Sigma^{-1}}}{\partial \sigma_i} X
\]
\[
  Q_{ij} = X^\top \frac{\partial{\Sigma^{-1}}}{\partial \sigma_i} \Sigma \frac{\partial{\Sigma^{-1}}}{\partial \sigma_j} X
\]
and $W_{ij}$ is the $(i, j)$ element of $V(\hat\sigma)$.

Further consider the bias of $\hat\Phi$ as an estimator of $\Phi$ using Taylor expansion

\[
  \hat\Phi \simeq \Phi + \sum_{i=1}^r{(\hat\sigma_i - \sigma_i)\frac{\partial{\Phi}}{\partial{\sigma_i}}} + \frac{1}{2} \sum_{i=1}^r{\sum_{j=1}^r{(\hat\sigma_i - \sigma_i)(\hat\sigma_j - \sigma_j)\frac{\partial^2{\Phi}}{\partial{\sigma_i}\partial{\sigma_j}}}}
\]

Ignoring the bias and the expectation is
\begin{equation}
  E(\hat\Phi) \simeq \Phi + \frac{1}{2} \sum_{i=1}^r{\sum_{j=1}^r{W_{ij}\frac{\partial^2{\Phi}}{\partial{\sigma_i}\partial{\sigma_j}}}} (\#eq:phi)
\end{equation}

Using previous notations,

\[
  \frac{\partial^2{\Phi}}{\partial{\sigma_i}\partial{\sigma_j}} = \Phi (P_i \Phi P_j + P_j \Phi P_i - Q_{ij} - Q_{ji} + R_{ij}) \Phi
\]
where
\[
  R_{ij} = X^\top\Sigma^{-1}\frac{\partial^2\Sigma}{\partial{\sigma_i}\partial{\sigma_j}} \Sigma^{-1} X
\]


Using \@ref(eq:lambda) and \@ref(eq:phi) to substitute \@ref(eq:vbeta) we have
\[
  V(\beta) = \hat\Phi_A = \hat\Phi + 2\hat\Phi \left\{\sum_{i=1}^r{\sum_{j=1}^r{W_{ij}(Q_{ij} - P_i \hat\Phi P_j - \frac{1}{4}R_{ij})} }\right\} \hat\Phi
\]

(Note: in some implementations of KR, $R_{ij}$ is ignored and leads to a variant.)

$W$ is the can be estimated using the inverse of information matrix by substituting the observed values

\[
  2I_{ij} = tr(\frac{\partial{\Sigma^{-1}}}{\partial \sigma_i} \Sigma \frac{\partial{\Sigma^{-1}}}{\partial \sigma_j} \Sigma) - tr(2\Phi Q_{ij} - \Phi P_i\Phi P_j)
\]

or, we can use the observed hessian matrix while fitting the model here. 

### Inference and degree of freedom

Suppose we are testing the $l$ linear combination of $\beta$, $L\beta$, we can use the following Wald-type statistic

\[
  F = \frac{1}{l} (\hat\beta - \beta)^\top  L (L^\top \hat\Phi_A L)^{-1} L^\top (\hat\beta - \beta)
\]
and 
\[
  F^* = \lambda F
\]
follows exact $F_{l,m}$ distribution.

$m$ can be calculated through

\[
  \Theta = L (L^\top \Phi L)^{-1} L^\top
\]

\[
  A_1 = \sum_{i=1}^r{\sum_{j=1}^r{W_{ij} tr(\Theta \Phi P_i \Phi) tr(\Theta \Phi P_j \Phi)}}
\]

\[
  A_2 = \sum_{i=1}^r{\sum_{j=1}^r{W_{ij} tr(\Theta \Phi P_i \Phi \Theta \Phi P_j \Phi)}}
\]

\[
  B = \frac{1}{2l}(A_1 + 6A_2)
\]

\[
  g = \frac{(l+1)A_1 - (l+4)A_2}{(l+2)A_2}
\]

\[
  c_1 = \frac{g}{3l+2(1-g)}
\]

\[
  c_2 = \frac{l-g}{3l+2(1-g)}
\]

\[
  c_3 = \frac{l+2-g}{3l+2(1-g)}
\]
\[E^*={\left\{1-\frac{A_2}{l}\right\}}^{-1}\]
\[V^*=\frac{2}{l}{\left\{\frac{1+c_1 B}{(1-c_2 B)^2(1-c_3 B)}\right\}}\]

\[\rho = \frac{V^{*}}{2(E^*)^2}\]

\[m = 4 + \frac{l+2}{l\rho - 1}\]
\[\lambda = \frac{m}{E^*(m-2)}\]


## mathmatical details for KR2

to be added.

## implementations

### degree of freedom derivation

```{r}
tr <- function(x) {
  sum(diag(x))
}
h_kr_df <- function(v0, va, l, w, p) { # p list of matrix, w matrix
  theta <- l %*% solve(t(l) %*% v0 %*% l) %*% t(l)
  nl <- ncol(l)
  thetav0 <- theta %*% v0
  thetav0pv0 <- lapply(p, function(x) {thetav0 %*% x %*% v0})
  a1 <- 0
  a2 <- 0
  for (i in seq_len(length(p))) {
    for (j in seq_len(length(p))) {
      a1 <- a1 + w[i, j] * tr(thetav0pv0[[i]]) * tr(thetav0pv0[[j]])
      a2 <- a2 + w[i, j] * tr(thetav0pv0[[i]] %*% thetav0pv0[[j]])
    }
  }
  b <- 1 / (2 * nl) * (a1 + 6 * a2)
  e <- 1 + a2 / nl
  e_star <- 1 / (1 - a2 / nl)
  g <- ((nl + 1) * a1 - (nl + 4) * a2) / ((nl + 2) * a2)
  denom <- (3 * nl + 2 - 2 * g)
  c1 <- g / denom
  c2 <- (nl - g) / denom
  c3 <- (nl + 2 - g) / denom
  v_star <- 2 / nl * (1 + c1 * b) / (1 - c2 * b)^2 / (1 - c3 * b)
  rho <- v_star / (2 * e_star^2)
  m <- 4 + (nl + 2)  / (nl * rho - 1)
  lambda <- m / (e * (m - 2))
  return(list(m = m, lambda = lambda))
}
```

### adjusted covariance and w, p, q, etc
```{r}
v_a <- function(v, w, p, q, r) {
  dr <- ncol(v)
  ret <- v
  for (i in seq_len(dr)) {
    for (j in seq_len(dr)) {
      ret <- ret + 2 * w[i, j] * v %*% (q[[i]][[j]] - p[[i]] %*% v %*% p[[j]] - 1 / 4 * r[[i]][[j]]) %*% v
    }
  }
  return(ret)
}

derivative <- function(sigma, r, order = 1) {
  # r: number of parameters
  # sigma: covariance matrix
  # order: order of derivative
  # provided the covariance structure, derive the first order derivatives
  # return a list of matrix
  return()
}

# if subject do not have all visits.
# L is added to sigma: sigmai = L %*% sigma %*% L
# the inverse derivative can be calculated samely.
# L is p * q, sigma is q * q (full visits)
# if L is identity, everything is fine
inverse_derivative <- function(sigma_inv, dsigma) {
  ret <- lapply(dsigma, function(x) {
    -sigma_inv %*% x %*% sigma_inv
  })
  return(ret)
}

r <- function(x, sigma_inv, sigma_deri_order2) {
  lapply(sigma_deri_order2, function(s) {
    lapply(s, function(t) {
      t(x) %*% sigma_inv %*% t %*% sigma_inv %*% x
    })
  })
}
p <- function(x, inv_sig_deri) {
  lapply(inv_sig_deri, function(s) { t(x) %*% s %*% x})
}

q <- function(x, sigma, inv_sig_deri) {
  lapply(inv_sig_deri, function(s) {
    lapply(inv_sig_deri, function(k) {
      t(x) %*% s %*% sigma %*% k %*% x
    })
  })
}

w <- function(inv_sig_deri, sigma, p, q, v) {
  r <- length(p)
  ret <- matrix(0, ncol = r, nrow = r)
  for (i in seq_len(r)) {
    for (j in seq_len(r)) {
      ret[i, j] <- tr(inv_sig_deri[[i]] %*% sigma %*% inv_sig_deri[[j]] %*% sigma) - tr(2 * v %*% q[[i]][[j]]) + tr(v %*% p[[i]] %*% v %*% p[[j]])
    }
  }
  return(ret)
}

```
## Special considerations for mmrm

For mmrm models, the covariance matrix $\Sigma$ is block diagonal: the residual of different timepoints for the same
subject is correlated, and for different subjects the residual are not correlated.

For mmrm models, the covariance matrix can be parameterized to reflect specific covariance structure, see [covariance structure](#covariance.html).

Since the covariance matrix is block diagonal, its derivatives and second order derivatives, are all block diagonal.

Hence, the implementations can be simplified to the sum of all subjects



## an actual case

```{r}
library(Matrix)
fit <- mmrm(FEV1 ~ ARMCD + ar1(AVISIT|USUBJID), data = fev_data)

derivative_ar1 <- function(visits, theta1, theta2, order = 1) {
  sig <- exp(2 * theta1)
  rho <- theta2 / sqrt(1 + theta2 ^ 2)
  rr <- (1 + theta2 ^ 2)
  # r: number of parameters
  # visits: number of total visits
  # sig: sigma^2
  # rho: rho
  # order: order of derivative
  # provided the covariance structure, derive the first order derivatives
  # return a list of matrix
  dis <- abs(outer(seq_len(visits), seq_len(visits), "-"))
  if (order == 0) {
    return(list(rho^dis * sig))
  }
  if (order == 1) {
    psig <- rho ^ dis * sig * 2
    prho <- rho ^ (dis - 1) * dis * sig * rr ^ -1.5
    return(list(sig = psig, rho = prho))
  }
  if (order == 2) {
    sigsig <- rho ^ dis * sig * 4
    sigrho <- rho ^ (dis - 1) * dis * sig * rr ^ -1.5 * 2
    rhosig <- rho ^ (dis - 1) * dis * sig * rr ^ -1.5 * 2
    rhorho <- rho ^ (dis - 2) * dis * (dis - 1) * sig * rr ^ -3 + rho ^ (dis - 1) * dis * sig * -3 * theta2 * rr ^ -2.5
    return(list(sig = list(sig = sigsig, rho = sigrho), rho = list(sig = rhosig, rho = rhorho)))
  }
}

# derivatives for 4 visits
d1 <- derivative_ar1(component(fit, "n_timepoints"), (fit$theta_est[1]), (fit$theta_est[2]), order = 1)
d2 <- derivative_ar1(component(fit, "n_timepoints"), (fit$theta_est[1]), (fit$theta_est[2]), order = 2)
# all subjects x
x <- component(fit, 'x')

l <- function(n_visits, index_zero) {
  m <- diag(rep(1, n_visits))
  m[index_zero + 1, , drop = FALSE]
}
l_matrix <- function(fit) {
  szi <- fit$tmb_data$subject_zero_inds
  vzi <- fit$tmb_data$visits_zero_inds
  szi_start <- szi + 1
  szi_end <- c(szi[-1], length(vzi))
  n_visits <- component(fit, "n_timepoints")
  lapply(seq_len(length(szi_start)), function(i) {
    l(n_visits, vzi[szi_start[i]:szi_end[i]])
  })
}
# l matrix for each subject
l_all <- l_matrix(fit)
l_sparse <- .bdiag(l_all)
# sigma for each subject
sigma_sparse <- l_sparse %*% .bdiag(rep(list(fit$cov), component(fit, "n_subjects"))) %*% t(l_sparse)
# sigma inverse for each subject
sigma_inv_sparse <- solve(sigma_sparse)
pars <- c(sig = "sig", rho = "rho")

# sigma derivative for each subject
sigma_d1_sparse <- lapply(pars, function(x) {
  l_sparse %*% .bdiag(rep(d1[x], component(fit, "n_subjects"))) %*% t(l_sparse)
})

sigma_inv_d1 <- inverse_derivative(sigma_inv_sparse, sigma_d1_sparse)

sigma_d2_sparse <- lapply(pars, function(x) {
  lapply(pars, function(d) {l_sparse %*% .bdiag(rep(list(d2[[x]][[d]]), component(fit, "n_subjects"))) %*% t(l_sparse)})
})

r_sparse <- r(x, sigma_inv_sparse, sigma_d2_sparse)
p_sparse <- p(x, sigma_inv_d1)
q_sparse <- q(x, sigma_sparse, sigma_inv_d1)
# w(sigma_inv_d1, sigma_sparse, p_sparse, q_sparse, fit$beta_vcov)/ 2
# should give identical results but here not
w_sparse <- solve(fit$tmb_object$he(fit$theta_est))
v_adjust <- v_a(fit$beta_vcov, w_sparse, p_sparse, q_sparse, r_sparse)
contrast <- matrix(c(0, 1), ncol = 1)
# degree of freedom should be very close now
df_adjust <- h_kr_df(fit$beta_vcov, v_adjust, contrast, w_sparse, p_sparse)

attr(v_adjust, "P") <- p_sparse
attr(v_adjust, "W") <- w_sparse

pbkrtest::Lb_ddf(t(contrast), fit$beta_vcov, v_adjust)
pbkrtest::ddf_Lb(v_adjust, contrast, fit$beta_vcov)

f <- fit$beta_est %*% contrast %*% solve(t(contrast) %*% fit$beta_vcov %*% contrast) %*% t(contrast) %*% matrix(fit$beta_est, ncol = 1)
f_star <- f* df_adjust$lambda
df(f_star, df1 = 1, df2 = df_adjust$m)
```

## using autodiff in C++ side

\[
  \frac{\partial{\Sigma}}{\partial{\theta}} = \frac{\partial{LL^\top}}{\partial{\theta}} = 2\frac{\partial{L}}{\partial{\theta}}L^\top
\]

\[
  \frac{\partial^2{\Sigma}}{\partial{\theta}^2} = 2\frac{\partial^2{L}}{\partial{\theta}^2}L^\top + 2\frac{\partial{L}}{\partial{\theta}}\frac{\partial{L^T}}{\partial{\theta}}
\]


```{c++}
template <class T>
struct sigma_at_ij {
  int i;
  int j;
  sigma_at_ij(int i, int j): i(i), j(j) {};
  const T operator() (const vector<T> &theta) {
    matrix<T> li = get_cov_lower_chol_grouped(theta, );
    return (li*li.transpose())(this->i, this->j); //i and j must be specified because hessian require scaler
  }
};

template <class T>
struct sigma_at_i {
  int i;
  sigma_at_ij(int i): i(i) {};
  const T operator() (const vector<T> &theta) {
    matrix<T> li = get_cov_lower_chol_grouped();
    return (li*li.transpose()).row(this->i); //i and j must be specified
  }
};


template<class Type>
Type objective_function<Type>::operator() ()
{
  // changes here
  matrix<Type> sigma_inv = matrix<Type>::Zero(x_matrix.rows(), x_matrix.rows()) // n * n
  matrix<Type> sigma_derivative1 = matrix<Type>::Zero(x_matrix.rows(), x_matrix.rows() * theta.size()) // n * n * theta_size to hold all derivatives
  matrix<Type> sigma_derivative2 = matrix<Type>::Zero(x_matrix.rows(), x_matrix.rows() * theta.size() * theta.size()) // n * n * theta_size^2 to hold second order derivatives
    // for each subject, add the derivatives, inverse
    matrix<Type> inv = sigma.inverse();
    sigma_inv.block() = inv;
    for (ii = 0; ii < theta.size(); ii++) {
      matrix<Type> d1 = autodiff::jacobian(sigma_at_ij(ii), theta);
      sigma_derivative1.block() = d1;
      for (jj = 0; jj < theta.size(); jj++) {
        struct f = f(n_visits, ii, jj); // get the i,j element of sigma      
        matrix<Type> d2 = autodiff::hessian(sigma_at_ij(ii, jj), theta);
        sigma_derivative2.block() = d2; 
        // I fear that conducting this will also be time consuming because there are too many calculations!
        // for each element we need to calculate the simga and find its derivatives
      }
    }
  return neg_log_lik;
}


```

# Reference

1. Kenward, Michael G., and James H. Roger. "Small sample inference for fixed effects from restricted maximum likelihood." Biometrics (1997): 983-997.
1. Kenward, Michael G., and James H. Roger. "An improved approximation to the precision of fixed effects from restricted maximum likelihood." Computational Statistics & Data Analysis 53.7 (2009): 2583-2595.
1. [SAS User Guide](https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.3/statug/statug_mixed_references.htm#statug_mixedkenw_m09)
1. [lmerTestR implementation](https://github.com/runehaubo/lmerTestR/blob/master/pkg_notes/implementation.Rmd)
1. [PBKRTEST webpage](https://people.math.aau.dk/~sorenh/software/pbkrtest/)
1. Kackar, Raghu N., and David A. Harville. "Approximations for standard errors of estimators of fixed and random effects in mixed linear models." Journal of the American Statistical Association 79.388 (1984): 853-862.