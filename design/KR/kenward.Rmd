---
title: "Details of the Kenward-Roger degree of freedom"
package: mmrm
output:
  rmarkdown::html_document:
          theme: "spacelab"
          highlight: "kate"
          toc: true
          toc_float: true
vignette: |
  %\VignetteIndexEntry{Details of the Kenward-Roger degree of freedom}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Here we describe the the algorithms of Kenward-Roger degree of freedom (and adjusted covariance matrix).

## Model definition

The model definition is the same as what we have in [Details of the model fitting in `mmrm`](algorithms.html).
We are using the same notations.

### Linear model

For each subject $i$ we observe a vector
\[
Y_i = (y_{i1}, \dotsc, y_{im_i})^\top \in \mathbb{R}^{m_i}
\]
and given a design matrix
\[
X_i \in \mathbb{R}^{m_i \times p}
\]
and a corresponding coefficient vector $\beta \in \mathbb{R}^{p}$ we assume
that the observations are multivariate normal distributed:
\[
Y_i \sim N(X_i\beta, \Sigma_i)
\]
where the covariance matrix $\Sigma_i \in \mathbb{R}^{m_i \times m_i}$ is derived
by subsetting the overall covariance matrix $\Sigma \in \mathbb{R}^{m \times m}$
appropriately by
\[
\Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
\]
where the subsetting matrix $S_i \in \{0, 1\}^{m \times m_i}$ contains
in each of its $m_i$ columns contains a single 1 indicating which overall time point
is matching $t_{ij}$. 
$G_i \in \mathbb{R}_{\gt 0}^{m_i \times m_i}$ is the diagonal weight matrix.

Conditional on the design matrices $X_i$, the coefficient vector $\beta$ and the
covariance matrix $\Sigma$ we assume that the observations are independent between
the subjects.

We can write the linear model for all subjects together as
\[
Y = X\beta + \epsilon
\]
where $Y \in \mathbb{R}^N$ combines all subject specific observations vectors $Y_i$
such that we have in total $N = \sum_{i = 1}^{n}{m_i}$ observations,
$X \in \mathbb{R}^{N \times p}$ combines all subject specific design matrices
and $\epsilon \in \mathbb{R}^N$ has a multivariate normal distribution
\[
\epsilon \sim N(0, \Omega)
\]
where $\Omega \in \mathbb{R}^{N \times N}$ is block-diagonal containing the
subject specific $\Sigma_i$ covariance matrices on the diagonal and 0 in the
remaining entries.

## Mathematical Details of Kenward-Roger method

The mathematical derivation of the Kenward-Roger method is based on the Taylor expansion of the obtained covariance matrix
of $\hat\beta$ to get a more accurate estimate for it. All these derivations are based on the restricted maximum likelihood.
Following the same [notation](algorithms.html#covariance-matrix-model), the covariance matrix, $\Omega$ can be represented as
a function of covariance matrix parameters $\theta = (\theta_1, \dotsc, \theta_k)^\top$, i.e. $\Omega(\theta)$.
Here after model fitting with `mmrm`, we obtain the estimate $\hat\beta = \Phi(\hat\theta)X^\top\Omega(\hat\theta)^{-1}Y$,
where $\Phi(\theta) = \left\{X^\top \Omega(\theta)^{-1} X\right\} ^{-1}$ is the asymptotic covariance matrix of $\hat\beta$.
However, Kackar and Harville(@kackar1984) suggests that although the $\hat\beta$ is unbiased for $\beta$, the covariance matrix,
$V[\hat\beta] = \left\{X^\top \hat\Omega X\right\}^{-1}$ can be biased. They showed that the variability of $\hat\beta$ can be partitioned into
two components,

\[
  V[\hat\beta] = \Phi + \Lambda
\]

where $\Phi$ is the variance-covariance matrix of the asymptotic distribution of $\hat\beta$ as $n\rightarrow \infty$, $\Lambda$ represents the amount
to which the asymptotic variance-covariance matrix underestimates $V[\hat\beta]$.

Based on a Taylor series expansion around $\theta$, $\Lambda$ can be approximated by

\[
  \Lambda \simeq \Phi \left\{\sum_{j=1}^r{\sum_{k=1}^r{W_{jk}(Q_{jk} - P_j \Phi P_k)} }\right\} \Phi
\]
where
\[
  P_j = X^\top \frac{\partial{\Omega^{-1}}}{\partial \theta_j} X
\]
\[
  Q_{jk} = X^\top \frac{\partial{\Omega^{-1}}}{\partial \theta_j} \Omega \frac{\partial{\Omega^{-1}}}{\partial \theta_k} X
\]

Again, based on a Taylor series expansion about $\theta$, Kenward and Roger(@kenward1997) show that
\[
  \hat\Phi \simeq \Phi + \sum_{j=1}^r{(\hat\theta_j - \theta_j)\frac{\partial{\Phi}}{\partial{\theta_j}}} + \frac{1}{2} \sum_{j=1}^r{\sum_{k=1}^r{(\hat\theta_j - \theta_j)(\hat\theta_k - \theta_k)\frac{\partial^2{\Phi}}{\partial{\theta_j}\partial{\theta_k}}}}
\]
Ignoring the possible bias in $\hat\theta$,
\[
  E(\hat\Phi) \simeq \Phi + \frac{1}{2} \sum_{j=1}^r{\sum_{k=1}^r{W_{jk}\frac{\partial^2{\Phi}}{\partial{\theta_j}\partial{\theta_k}}}}
\]
Using previously defined notations, this can be further written as
\[
  \frac{\partial^2{\Phi}}{\partial{\theta_j}\partial{\theta_k}} = \Phi (P_j \Phi P_k + P_k \Phi P_j - Q_{jk} - Q_{kj} + R_{jk}) \Phi
\]
where
\[
  R_{jk} = X^\top\Omega^{-1}\frac{\partial^2\Omega}{\partial{\theta_j}\partial{\theta_k}} \Omega^{-1} X
\]

substituting $\Phi$ and $\Lambda$ back to the $V[\hat\beta]$, we have

\[
  V(\hat\beta) = \hat\Phi + 2\hat\Phi \left\{\sum_{j=1}^r{\sum_{k=1}^r{W_{jk}(Q_{jk} - P_j \hat\Phi P_k - \frac{1}{4}R_{jk})} }\right\} \hat\Phi
\]

where $\Omega(\hat\theta)$ replaces $\Omega(\theta)$ in the righthand side, and $W$ is the inverse of the Hessian matrix of the objective function.

### Special Considerations for mmrm models

In mmrm models, $\Omega$ is a block-diagonal matrix, hence we can calculate  $P$, $Q$ and $R$ for each subject and add them up.

\[
  P_j = \sum_{i=1}^{N}{P_{ij}} = \sum_{i=1}^{N}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} X_i}
\]

\[
  Q_{jk} = \sum_{i=1}^{N}{Q_{ijk}} = \sum_{i=1}^{N}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} \Sigma_i \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_k} X_i}
\]

\[
  R_{jk} = \sum_{i=1}^{N}{R_{ijk}} = \sum_{i=1}^{N}{X_i^\top\Sigma_i^{-1}\frac{\partial^2\Sigma_i}{\partial{\theta_j}\partial{\theta_k}} \Sigma_i^{-1} X_i}
\]


### Derivative of the overall covariance matrix $\Sigma$

The derivative of the overall covariance matrix $\Sigma$ with respect to the variance parameters can be calculated through the derivatives of the Cholesky factor, and hence obtained through automatic differentiation:

\[
  \frac{\partial{\Sigma}}{\partial{\theta_j}} = \frac{\partial{LL^\top}}{\partial{\theta_j}} = \frac{\partial{L}}{\partial{\theta_j}}L^\top + L\frac{\partial{L^\top}}{\partial{\theta_j}}
\]

\[
  \frac{\partial^2{\Sigma}}{\partial{\theta_j}\partial{\theta_k}} = \frac{\partial^2{L}}{\partial{\theta_j}\partial{\theta_k}}L^\top + L\frac{\partial^2{L^\top}}{\partial{\theta_j}\partial{\theta_k}} + \frac{\partial{L}}{\partial{\theta_j}}\frac{\partial{L^T}}{\partial{\theta_k}} + \frac{\partial{L}}{\partial{\theta_k}}\frac{\partial{L^\top}}{\partial{\theta_j}}
\]

### Derivative of the $\Sigma^{-1}$

The derivative of sigma inverse can be calculated through

\[
  \frac{\partial{\Sigma\Sigma^{-1}}}{\partial{\theta}}\\
  = \frac{\partial{\Sigma}}{\partial{\theta}}\Sigma^{-1} + \Sigma\frac{\partial{\Sigma^{-1}}}{\partial{\theta}} \\
  = 0
\]
\[
  \frac{\partial{\Sigma^{-1}}}{\partial{\theta}} = - \Sigma^{-1} \frac{\partial{\Sigma}}{\partial{\theta}}\Sigma^{-1}
\]

### Subjects with missed visits

If a subject do not have all visits, the corresponding covariance matrix can be represented as
\[
  \Sigma_i = S_i^\top \Sigma S_i
\]

and the derivatives can be obtained through

\[
  \frac{\partial{\Sigma_i}}{\partial{\theta}} = S_i^\top \frac{\partial{\Sigma}}{\partial{\theta}} S_i
\]

\[
  \frac{\partial^2{\Sigma_i}}{\partial{\theta_j}\partial{\theta_k}} = S_i^\top \frac{\partial^2{\Sigma}}{\partial{\theta_j}\partial{\theta_k}} S_i
\]

The derivative of the $\Sigma_i^{-1}$, $\frac{\partial\Sigma_i^{-1}}{\partial{\theta}}$ can be calculated through $\Sigma_i^{-1}$ and $\frac{\partial{\Sigma_i}}{\partial{\theta}}$
using section [Derivative of the sigma inverse](#derivative-of-the-sigma-inverse).

### Scenario under group specific covariance estimates

When conducting grouped `mmrm` models, the covariance matrix for subject i of group $g(i)$, can be written as $\Sigma_{g(i)}$.
Assume there are $B$ groups, the number of parameters is increased by $B$ times. With the fact that for each group, the corresponding
$\theta$ will not affect other parts, we will have block-diagonal $P$, $Q$ and $R$ matrix.

\[
P_j = \begin{pmatrix}
P_{j, 1} & \dots & P_{j, B} \\
\end{pmatrix}
\]

\[
Q_j = \begin{pmatrix}
Q_{j, 1} & 0 & \dots & \dots & 0 \\
0 & Q_{j, 2} & 0 & \dots & 0\\
\vdots & & \ddots & & \vdots \\
\vdots & & & \ddots & \vdots \\
0 & \dots & \dots & 0 & Q_{j, B}
\end{pmatrix}
\]

Use $P_{j, b}$ to denote the block diagonal part for group $b$, we have
\[
  P_{j,b} = \sum_{g(i) = b}{P_{ij}} = \sum_{g(i) = b}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} X_i}
\]

\[
  Q_{j,b} = \sum_{g(i) = b}{Q_{ijk}} = \sum_{g(i) = b}{X_i^\top \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_j} \Sigma_i \frac{\partial{\Sigma_i^{-1}}}{\partial \theta_k} X_i}
\]


Similarly for $R$.

### Scenario under weighted mmrm

Under weights mmrm model, the covariance matrix for subject $i$, can be represented as

\[
  \Sigma_i = G_i^{-1/2} S_i^\top \Sigma S_i G_i^{-1/2}
\]

Where $G_i$ is a diagonal matrix of the weights. Then, when deriving $P$, $Q$ and $R$,
there are no mathematical differences as they are constant, and having $G_i$ in addition to $S_i$
does not change the algorithms and we can simply multiply the formulas with $G_i^{-1/2}$, similarly as above for the subsetting matrix.

## Inference

Suppose we are testing the linear combination of $\beta$, $C\beta$ with $C \in \mathbb{R}^{c\times p}$, we can use the following Wald-type statistic

\[
  F = \frac{1}{c} (\hat\beta - \beta)^\top  C (C^\top \hat\Phi_A C)^{-1} C^\top (\hat\beta - \beta)
\]
and
\[
  F^* = \lambda F
\]
follows exact $F_{c,m}$ distribution.

$\lambda$ and $m$ can be calculated through

\[
  M = C (C^\top \Phi C)^{-1} C^\top
\]

\[
  A_1 = \sum_{j=1}^r{\sum_{k=1}^r{W_{jk} tr(M \Phi P_j \Phi) tr(M \Phi P_k \Phi)}}
\]

\[
  A_2 = \sum_{j=1}^r{\sum_{k=1}^r{W_{jk} tr(M \Phi P_j \Phi M \Phi P_k \Phi)}}
\]

\[
  B = \frac{1}{2c}(A_1 + 6A_2)
\]

\[
  g = \frac{(c+1)A_1 - (c+4)A_2}{(c+2)A_2}
\]

\[
  c_1 = \frac{g}{3c+2(1-g)}
\]

\[
  c_2 = \frac{c-g}{3c+2(1-g)}
\]

\[
  c_3 = \frac{c+2-g}{3c+2(1-g)}
\]
\[E^*={\left\{1-\frac{A_2}{c}\right\}}^{-1}\]
\[V^*=\frac{2}{c}{\left\{\frac{1+c_1 B}{(1-c_2 B)^2(1-c_3 B)}\right\}}\]

\[\rho = \frac{V^{*}}{2(E^*)^2}\]

\[m = 4 + \frac{c+2}{c\rho - 1}\]
\[\lambda = \frac{m}{E^*(m-2)}\]
